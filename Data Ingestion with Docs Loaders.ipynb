{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f07e5b-7059-429a-932b-0a434f5e8864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
      "  Using cached langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.10-cp312-none-win_amd64.whl.metadata (51 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.1)\n",
      "Using cached langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.13-py3-none-any.whl (408 kB)\n",
      "Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.137-py3-none-any.whl (296 kB)\n",
      "Using cached orjson-3.10.10-cp312-none-win_amd64.whl (139 kB)\n",
      "Installing collected packages: orjson, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed langchain-0.3.4 langchain-core-0.3.13 langchain-text-splitters-0.3.0 langsmith-0.1.137 orjson-3.10.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806dfbb5-1b8b-41d2-b24e-5b030b671728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.4)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.13)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (0.1.137)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, marshmallow, dataclasses-json, pydantic-settings, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.3 marshmallow-3.23.0 pydantic-settings-2.6.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b39ff1-db5b-41e8-ac81-c2a1e9d302d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x280a0aa67e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('Speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057e0b42-9da2-46b0-9839-17b69bdbb276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Speech.txt'}, page_content='Introduction\\nDemocracy is mainly a Greek word which means people and their rules, here peoples have the to select their own government as per their choice. Greece was the first democratic country in the world. India is a democratic country where people select their government of their own choice, also people have the rights to do the work of their choice. There are two types of democracy: direct and representative and hybrid or semi-direct democracy. There are many decisions which are made under democracies. People enjoy few rights which are very essential for human beings to live happily. \\n\\nOur country has the largest democracy. In a democracy, each person has equal rights to fight for development. After the independence, India has adopted democracy, where the people vote those who are above 18 years of age, but these votes do not vary by any caste; people from every caste have equal rights to select their government. Democracy, also called as a rule of the majority, means whatever the majority of people decide, it has to be followed or implemented, the representative winning with the most number of votes will have the power. We can say the place where literacy people are more there shows the success of the democracy even lack of consciousness is also dangerous in a democracy. Democracy is associated with higher human accumulation and higher economic freedom. Democracy is closely tied with the economic source of growth like education and quality of life as well as health care. The constituent assembly in India was adopted by Dr B.R. Ambedkar on 26th November 1949 and became sovereign democratic after its constitution came into effect on 26 January 1950.\\n\\n \\n\\nWhat are the Challenges:\\nThere are many challenges for democracy like- corruption here, many political leaders and officers who donâ€™t do work with integrity everywhere they demand bribes, resulting in the lack of trust on the citizens which affects the country very badly. Anti-social elements- which are seen during elections where people are given bribes and they are forced to vote for a particular candidate. Caste and community- where a large number of people give importance to their caste and community, therefore, the political party also selects the candidate on the majority caste. We see wherever the particular caste people win the elections whether they do good for the society or not, and in some cases, good leaders lose because of less count of the vote.\\n\\nIndia is considered to be the largest democracy around the globe, with a population of 1.3 billion. Even though being the biggest democratic nation, India still has a long way to becoming the best democratic system. The caste system still prevails in some parts, which hurts the socialist principle of democracy. Communalism is on the rise throughout the globe and also in India, which interferes with the secular principle of democracy. All these differences need to be set aside to ensure a thriving democracy.\\n\\n \\n\\nPrinciples of Democracy:\\nThere are mainly five principles like- republic, socialist, sovereign, democratic and secular, with all these quality political parties will contest for elections. There will be many bribes given to the needy person who require food, money, shelter and ask them to vote whom they want. But we can say that democracy in India is still better than the other countries.\\n\\nBasically, any country needs democracy for development and better functioning of the government. In some countries, freedom of political expression, freedom of speech, freedom of the press, are considered to ensure that voters are well informed, enabling them to vote according to their own interests.\\n\\n \\n\\nLet us Discuss These Five Principles in Further Detail\\nSovereign: In short, being sovereign or sovereignty means the independent authority of a state. The country has the authority to make all the decisions whether it be on internal issues or external issues, without the interference of any third party.\\n\\nSocialist: Being socialist means the country (and the Govt.), always works for the welfare of the people, who live in that country. There should be many bribes offered to the needy person, basic requirements of them should be fulfilled by any means. No one should starve in such a country.\\n\\nSecular: There will be no such thing as a state religion, the country does not make any bias on the basis of religion. Every religion must be the same in front of the law, no discrimination on the basis of someoneâ€™s religion is tolerated. Everyone is allowed to practice and propagate any religion, they can change their religion at any time.\\n\\nRepublic: In a republic form of Government, the head of the state is elected, directly or indirectly by the people and is not a hereditary monarch. This elected head is also there for a fixed tenure. In India, the head of the state is the president, who is indirectly elected and has a fixed term of office (5 years).\\n\\nDemocratic: By a democratic form of government, means the countryâ€™s government is elected by the people via the process of voting. All the adult citizens in the country have the right to vote to elect the government they want, only if they meet a certain age limit of voting.\\n\\n \\n\\nMerits of Democracy:\\nbetter government forms because it is more accountable and in the interest of the people.\\n\\nimproves the quality of decision making and enhances the dignity of the citizens.\\n\\nprovide a method to deal with differences and conflicts.\\n\\nA democratic system of government is a form of government in which supreme power is vested in the people and exercised by them directly or indirectly through a system of representation usually involving periodic free elections. It permits citizens to participate in making laws and public policies by choosing their leaders, therefore citizens should be educated so that they can select the right candidate for the ruling government. Also, there are some concerns regarding democracy- leaders always keep changing in democracy with the interest of citizens and on the count of votes which leads to instability. It is all about political competition and power, no scope for morality.\\n\\n \\n\\nFactors Affect Democracy:\\nculture\\n\\ncapital and civil society\\n\\neconomic development\\n\\nequality\\n\\nmodernization\\n\\nNorway and Iceland are the best democratic countries in the world. India is standing at fifty-one position.\\n\\nIndia is a parliamentary democratic republic where the President is head of the state and Prime minister is head of the government. The guiding principles of democracy such as protected rights and freedoms, free and fair elections, accountability and transparency of government officials, citizens have a responsibility to uphold and support their principles. Democracy was first practised in the 6th century BCE, in the city-state of Athens. One basic principle of democracy is that people are the source of all the political power, in a democracy people rule themselves and also respect given to diverse groups of citizens, so democracy is required to select the government of their own interest and make the nation developed by electing good leaders.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Converting Text to document\n",
    "text_document=loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d3b9fef-ac65-404e-a5dd-eeabe264f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50e670b8-ef7a-40bf-9ba7-0e6c51f5b014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 0}, page_content='Machine Learning  \\n \\nMachine learning is a subset of artificial intelligence (AI) that focuses on \\ndeveloping computer systems capable of learning and improving from data \\nwithout being explicitly programmed.  Machine learning is teaching computers to \\nlearn from data and make decisions or predictions based on that learning.  \\nMachine learning is widely used in various fields and applications  such as:  \\n\\uf0b7 Face Recognition  \\n\\uf0b7 Object Detection  \\n\\uf0b7 Chatbots  \\n\\uf0b7 Recommendation Systems  \\n\\uf0b7 Autonomous Vehicles  \\n\\uf0b7 Disease Diagnosis  \\n\\uf0b7 Fraud de tection  \\nAnd many more it is nowadays being used in almost all sectors including \\nhealthcare, education, business, construction, astronomy, etc.  \\n \\nTypes of Machine Learning : \\n \\nThere are many types of machine learning but the most famous types are:  \\n\\uf0d8 Supervised l earning  \\n\\uf0d8 Unsupervised learning  \\n\\uf0d8 Reinforcement learning  \\n \\n \\n \\n '),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 1}, page_content=\"Supervised Learning  \\n \\nIn supervised learning the algorithm is trained on labeled data which means \\ntraining data includes the input and output pairs. The goal is to learn the mapping \\nfrom input to outp ut. \\nX -> Y \\nSome examples of supervised learning include image classification, email spam \\ndetection, and predicting software.  \\n \\nTypes of supervised learning tasks:  \\n \\n1. Regression  \\n2. Classification  \\n \\nRegression:  \\nRegression is a type of supervised learning task where  the algorithm's goal is to \\npredict a continuous numerical output or target variable.  In regression, the output \\nis a real -valued number, and the algorithm's objective is to learn a mapping from \\ninput features to this continuous output.  \\nExamples of regressi on tasks include home price prediction, black hole mass \\nprediction, stock price prediction, age estimation, etc.  \\nAlgorithms:  \\n\\uf076 Linear Regression:  \\n \\nLinear Regression is a fundamental statistical and machine -learning  \\ntechnique  for solving regression problems  used for modeling the \\nrelationship between a dependent variable (or target) and one or more \"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 2}, page_content='independent variables (or features). It assumes that this relationship is \\napproximately linear, meaning that changes in the independent variables \\nhave a linear effec t on the dependent variable.  \\n\\uf0d8 Simple Linear Regression:  \\nIn simple linear regression, there is only one input feature.  \\nY= wx+b  \\nY= target variable  \\nW=slope of straight line  \\n X=input features  \\n b= y-intercept  \\n\\uf0d8 Multiple Linear Regression:  \\nIn multiple linear regression, there is more than one input feature.  \\nY=w1x1+w2x2+w3x3+ …. +b \\n \\n \\n \\n  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 3}, page_content=' Note: There are 2 common issues in machine learning and statistical \\nmodeling that are underfitting  and overfitting . \\nUnderfitting:  \\nUnderfitting occurs when a  model is too simple to capture the underlying patterns \\nin the data, both in the training set and unseen data.  It essentially means the \\nmodel is not complex enough to represent the relationships between the input \\nfeatures and the target variable.  This mean s it is not predicting the output \\ncorrectly and the accuracy of the model is very low.  Solutions to overcome this \\nproblem are:  \\n1) Increase model complexity.  \\n2) Add more features.  \\n3) Feature engineering (It is basically creating the new input feature using the \\nexisti ng input features For example for home price prediction you are given \\nthe width and length of a home you can simply create another feature \\nnamed area of home by multiplying the width and length of home you \\nalready know).  \\n4) Remove noise (clean data to remove  outliers).  \\n5) Increase training data.  \\n6) Increase training data.  \\n7) Reduce regularization.  \\n8) Iterating the optimization algorithms like gradient descent for more times.  \\nOverfitting:  \\nOverfitting occurs when a model is excessively complex and fits the training data \\nnoise rather than the underlying patterns.  It means the model is too flexible and \\nessentially memorizes the training data rather than generalizing it.  Simply we can \\nsay that in this case the cost function  (The cost function quantifies the error \\nbetween pre dicted and expected values and presents that error in the form of a \\nsingle real number.)   is very low close to 0 and the accuracy of the model is \\naround 100%.  Solutions to overcome this problem are:  \\n1) Collect more training data  \\n2) Feature selection  \\n3) Feature eng ineering  '),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 4}, page_content=\"4) Do regularization.  \\nRegularization:  \\nRegularization is a technique used in machine learning and statistics to prevent \\noverfitting, which occurs when a model fits the training data too closely, \\ncapturing noise and making it less effective at making p redictions on new, \\nunseen data. Regularization adds a penalty term to the model's loss function, \\ndiscouraging it from fitting the training data too precisely and encouraging it to \\nfind a simpler, more generalizable solution.  There are 2 of its common \\ntechniques.  \\nA. L1 regularization (lasso):  \\nL1 regularization adds the absolute values of the model's parameters to the \\nloss function.  Lasso is useful for feature selection and simplifying models.  \\n                \\nB. L2 regularization (r idge):  \\nL2 regularization adds the squared values of the model's parameters to the \\nloss function . Ridge helps reduce model complexity and is effective when all \\nfeatures are potentially relevant . \\n \\n \\n  \\nNote :  Learning rate  is a hyperparameter in machine learn ing algorithms, \\nparticularly in optimization algorithms like gradient descent. It determines the size \\nof steps that the algorithm takes when adjusting the model's parameters during \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 5}, page_content=\"training . Choosing a small learning rate can lead you to converge to the so lution \\nslowly and by choosing a large value of learning rate the solution does not \\nconverge to a point.  \\n \\nValidation set in Machine learning:  \\nA validation set is a portion of the dataset used in machine learning to assess the \\nperformance of a trained model.  It's like a practice exam for the model before it \\nfaces the real test (the test set).  The validation set is not part of the training data, \\nnor is it part of the final evaluation data (the test set). Instead, it serves as a \\nmiddle ground for testing the mo del's performance during training.  It helps you \\nmake decisions about how well your model is learning and whether it's overfitting \\nor underfitting.  During the training process, after each training epoch (iteration), \\nthe model's performance is evaluated on the validation set.  The model's \\npredictions on the validation set are compared to the actual target values, and a \\nperformance metric (like accuracy, loss, or other relevant metrics) is computed.  \\nThis metric helps you understand how well the model is doing on data it hasn't \\nseen during training.  You can adjust hyperparameters (like learning rate, model \\narchitecture, or regularization strength) based on the model's performance on the \\nvalidation set. For example, if the model's performance on the validation se t starts \\nto degrade, you may want to reduce the learning rate or simplify the model.  \\nFeature Scaling in Machine Learning:  \\nFeature scaling is a preprocessing technique in machine learning that helps bring \\nall the features (variables) of your dataset onto a similar scale. It ensures that no \\nsingle feature dominates the learning process because of its larger magnitude.  \\nDifferent features in your dataset may have values on different scales. For \\nexample, one feature might range from 0 to 1, while another might r ange from 0 \\nto 1000.  Feature scaling is used to make sure that these varying scales do not \\naffect the performance of machine learning algorithms.  Feature scaling involves \\ntransforming the values of each feature so that they fall within a similar numerical \\nrange.  The common techniques used to do feature scaling are min -max scaling \\nand z -score. Feature scaling ensures that all features contribute equally to the \\nlearning process, preventing some features from having undue influence due to \"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 6}, page_content=\"their larger values.  It can lead to faster convergence of gradient -based \\noptimization algorithms (like gradient descent).  \\n\\uf076 Polynomial Regression:  \\n \\nPolynomial regression is a type of regression analysis used in machine \\nlearning and statistics to model relationships between varia bles when the \\ntraditional linear regression model is insufficient. It extends the concept of \\nlinear regression by allowing for more complex, nonlinear relationships \\nbetween the independent and dependent variables.  \\ny = b ₀ + b₁x + b ₂x² + ... + bₙxⁿ \\n \\n \\n \\nThe degree of the polynomial (n) is a hyperparameter that you can choose \\nbased on the complexity of the relationship you want to capture.  \\nA higher degree allows the model to fit the data more closely but may also \\nlead to overfitting if not chosen carefully.  \\nIn order if overfitting occurs then reduce the degree of polynomial, collect \\nmore data do regularization.  \\n \\n\\uf076 KNN regression:  \\n \\nK-Nearest Neighbors (KNN) regression is a simple and intuitive machine \\nlearning algorithm used for regression tasks. It's an instance -based learning \\nmethod that makes predictions by considering the average (or weighted \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 7}, page_content='average) of the target values of its k -nearest neighbors in the training data.  \\nKNN regression is used to predict a continuous target variable (numeric \\nvalue) based on the values of its neighboring data points.  During training, \\nthe KNN algorithm stores the entire training dataset.  \\n \\n \\nTo make a prediction for a new data point, the algorithm identifies the k -\\nnearest data points (neighbors) in the training set based on a distan ce \\nmetric (usually Euclidean distance).  K is a hyperparameter here. The \\ndistance is calculated between the entered point from all points in the \\nwhole data set then the data set is sorted based on distance in ascending \\norder and picked the first k rows and the mean of target variables are \\ncalculated which is the answer. The value of k is a hyperparameter that you \\nneed to specify when using KNN regression.  A smaller k (e.g., 1 or 3) makes \\nthe model sensitive to noise in the data and can result in a more varia ble \\nprediction.  A larger k (e.g., 10 or 20) provides a smoother prediction but \\nmight not capture local patterns as effectively.  KNN regression is simple to \\nunderstand and implement.  It can capture complex and nonlinear \\nrelationships between features and th e target variable.  Choosing the \\nappropriate value of k is crucial and can be challenging.  KNN can be \\ncomputationally expensive when the dataset is large, as it requires \\ncalculating distances to all data points during prediction.  KNN regression \\nworks well f or small to moderately -sized  datasets with a reasonable number \\nof features.  For large datasets, the computational cost of finding nearest \\nneighbors can become prohibitive.  If overfitting occurs in KNN regression \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 8}, page_content=\"then do better feature selection, adjust the  value of k, do feature selection \\netc. \\n \\n\\uf076 Decision Tree Regressor/ Regression tree : \\n \\nA regression tree is a type of decision tree used in machine learning for \\nregression tasks. It's a tree -like structure that makes predictions about \\ncontinuous numeric values . Consider a scenario in which you are trying to \\npredict the drug usage effectiveness prediction.  \\n \\n \\nNow we have to build a tree and how should be built. Like what value of \\ndrug dosage should be the root node here is what we should know about \\nthe square sum residual. In the diagram let's focus on small 2 values and \\ntake an average of it that will be 3. No w imagine the node has a drug \\ndosage of less than 3. Now for all data below 3 calculate the average value \\nof effectiveness of all and also calculate the average value of effectiveness \\non data set above 3. Calculate the Squared sum residual in case of savin g it. \\nNow focus on the next 2 data points calculate the average of and consider \\nthe root to be valued less than 5 and do the same that we did for case of \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 9}, page_content='drug dosage less than 3 after that take the next data sets and do same. Now \\ncheck for the residuals yo u stored.  \\n                        \\n \\nAs seen select the dosage value with less residual value that is 14.5. Now \\nroot node will have a drug dosage of less than 14.5. Now after that, if the \\ndata sets less than 14.5 are really less values then no need to furth er split it \\njust calculate their average value and make it a leaf node. For the data set \\nabove 14.5  split it into other nodes using the same concept that we did \\nearlier selecting the threshold value calculating the residuals and selecting \\none with less re sidual. The final tree will be like,  \\n                 \\n \\nHere the scenario we discussed has only one input feature If there are \\nmultiple features calculate the residuals for each feature and then select \\none with less value of residual and make the tree.  They are e asy to \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 10}, page_content='understand and able  due to their tree -like structure.  It can  model complex, \\nnonlinear relationships in data.  Outliers have minimal impact on model \\nperformance.  But it is Sensitive to small changes in the data.  If overfitting \\nhappens in the regression tree then reduce the depth of the tree and do \\npruning.  \\nPruning:  \\nPruning is a technique used in decision tree -based models, including \\nregression trees, to prevent overfitting and improve model generalization.  \\nPruning  involves cutting back or removing some branches (subtrees) of a \\ndecision tree after it has been fully grown.  It aims to simplify the tree by \\nremoving branches that capture noise or fine -grained details in the training \\ndata.  Pruning techniques consider a c ost-complexity trade -off: They \\nevaluate the cost (error) associated with keeping or removing each subtree \\nand select the option that minimizes this cost.  Pruned subtrees are replaced \\nwith a single leaf node, often representing the average or majority class  (for \\nclassification) or the average value (for regression) of the training data in \\nthat subtree.  \\n \\n\\uf076 Random Forest Regression:  \\n \\nRandom Forest Regression is a machine learning method for predicting \\ncontinuous values, like house prices or temperatures . Random Forest \\nRegression is like having a group of decision  trees  who collaborate to make \\npredictions.  When making a prediction, the Random Forest collects \\nopinions from all the decision trees.  It combines these opinions by \\naveraging their predictions to arrive a t a final, more accurate prediction.  If \\noverfitting happens in random forest regression then reduce the number of \\ntrees, feature selection, pruning, etc. Random Forest Regression is less \\nprone to overfitting compared to a single Decision Tree.  Random Fores ts \\noften provide higher predictive accuracy compared to a single Decision \\nTree.  Random Forests are more stable and less sensitive to small variations \\nin the training data . While on the other hand,  Random Forests are more \\ncomplex than individual Decision Trees . Random Forests typically require '),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 11}, page_content=\"more computational resources and training time than a single Decision \\nTree . \\n \\n\\uf076 Support Vector Regressor:  \\n \\nSVR is a machine learning algorithm used for regression tasks. It's an extension \\nof Support Vector Machines (SVM s) that were originally designed for \\nclassification.  The data points outside the tube of E ( eta)  to the regression line. \\nThey have the most influence on the positioning of the regression line  these \\ndata points are called support vectors . So, in SVR we als o draw a marginal line \\nand our goal is to maximize the marginal line so maximum data points lie inside \\nthe marginal lines.  \\n \\n \\n                     \\n \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 12}, page_content='                                             \\n \\n So the cost function will be  \\n                                               \\nAnd our aim is to minimize the cost function.  Where c controls how many data \\npoints can be allowed to exist outside the margin (the region around the \\nregression line).  so a smaller value of c means that there  are fewer  values \\noutside the margin line and model  accuracy will be high and a high value of c \\nthat there will be more points outside the margin line and accuracy will be low . \\nWhile other parameter tells us about the distance between the data point \\noutsid e the marginal line and the marginal line. So here is the scenario we \\nreduce the cost function in the way and make a model. So in SVR   kernel is \\nbasically a function that transforms the  data into higher dimensions  in order to \\nbuild  a model SVR  that can h andle outliers effectively. Training an SVR model \\ncan be computationally expensive, especially on very large datasets.  In order to \\navoid overfitting in the Support vector regressor we select the proper value of \\nhyperparameters, feature selection, feature s caling, regularization, and kernel \\nselection.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 13}, page_content=' \\n\\uf076 XGBoost Regression:  \\n \\nXGBoost (eXtreme Gradient Boosting) is a powerful and popular machine -\\nlearning  algorithm known for its efficiency and high performance across a \\nwide range of tasks. It belongs to the ensemble learning category, which \\nmeans it combines the predictions of multiple base models (often decision \\ntrees) to improve overall performance.  Supp ose we have this data, and we \\nwant to predict the drug effectiveness.  \\n                                      \\nThe first step in fitting XGBoost to training data is to make an initial prediction \\nwhich is basically the mean of the target variable in the train ing data set. In the \\nabove example, the initial prediction is 0.5. Then we calculate the residuals \\nwhich is the difference between the observed and predicted value. So \\naccording to the above data set the residual values are : \\n                                         \\nNow we do calculations for similarity gain and the formula for this is,  \\n            \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 14}, page_content='Where lambda is a hyperparameter and we are assuming it is 0 here. Now we \\ncalculate the similarity gain of the root that is 0. Now for splitting checks which \\nwill be better. So, for this, we will take an average of the first 2 data sets and \\nthat is the average whose value is 15. Now make dosage less than 15 a root \\nnode and we will calculate a gain first we will calculate the similarity score of \\neach node.  \\n                                    \\n                                    \\n                             \\nWe calculated the similarity score now we will calculate the gain.  \\n                              \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 15}, page_content='So the gain is 120.33. We calculated the gain for a threshol d dosage of less \\nthan 15. Now we will take the other 2 data sets and calculate an average that is \\n22.5 Now we will calculate the gain for 22.5.  \\n                                          \\nThe gain of this node is 4. Now we will take other values and compute  the \\naverage for new threshold value that is dosage less than 30.  \\n              \\nNow we will select the threshold that will calculate the largest gain that is \\ndosage less than 15.  \\n                                           \\nNow as in left node there is only one residual we cannot split it further but we \\ncan split the right node. Now we will again do the same procedure as explained \\nabove. Now we will look into data for values greater than 15. The next node is \\nless than 22.5.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 16}, page_content='                                             \\nWe calculate the similarity score of node.  \\n                                                                    \\nAnd gain for the node is below.  \\n                                 \\nNow we calculate the gain of next threshold. That is,  \\n        \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 17}, page_content='As a dosage less than 30 has a high value of gain we will select this as a node.  \\n                                              \\nNow let ’s discuss the pruning of a tree for that we use a hyperparameter called \\ngamma. Pruning is done based on the value of ga in whether the subtree has to be \\nremoved or not. So if the difference between gain and gamma is negative we \\nremove the subtree we do not remove it. Now if we consider the value of gamma \\nas 130 initially and do 140.17 – 130 answer will be positive so we wil l not remove \\nthe subtree. For the root node, the difference is negative but we will not remove it \\nbecause we haven ’t removed the child nodes of the root node. Here is how \\npruning is done. We can also set the limit of the depth of a tree.  Now we will \\ncalcul ate the output value of each leaf node.  \\n                        let lambd a=0. \\n                                 \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 18}, page_content='                                             \\n                               \\nNow we can use the above tree  to make new predictions. For that the formula is,  \\n  Predicted value initial prediction + (learning rate * output value)  \\n \\nSo for data 13 dosages, the predicted value is,  \\n                                            \\nFor a dosage equal to 20, the predicted v alue is 2.6. Now we will repeat the steps \\nwith all values.  \\n                                    \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 19}, page_content='Now we will build another tree based on new residuals and make new predictions \\nthat give us even smaller residuals. V alues of predicted values  that we got from \\nthe first tree  then we used  these values and calculated their mean which is used \\nas the initial predictor for building the second tree and so on.  This algorithm \\npredicts highly accurate values. Training an XGBoost model with a large number of \\ntrees and dee p trees can be resource -intensive in terms of memory and \\ncomputation.  It is applicable to a wide range of data sets. If overfitting occurs then \\nwe do limit the number of trees, limit the depth of trees, etc.  \\n \\nClassification:  \\nClassification in supervised ma chine learning is like teaching a computer to \\nrecognize and sort things into different groups based on their unique \\ncharacteristics. It\\'s like how we classify objects in our daily lives.  \\nExample: Disease detection etc.  \\nAlgorithms:  \\n \\n\\uf076 Logistic Regression:  \\nLogistic regression is a type of statistical model used for classification tasks \\nin machine learning. It\\'s particularly useful when the target variable (what \\nyou\\'re trying to predict) is categorical. This means it can have only two \\npossible outcomes, such as \"yes\" or \"no\", \"spam\" or \"not spam\", etc.  Unlike \\nlinear regression, where the output can be any real number, logistic \\nregression outputs probabilities. These probabilities are constrained to be \\nbetween 0 and 1.  Logistic regression uses the logistic func tion (also known \\nas the sigmoid function) to model the relationship between the features \\nand the probability of a specific outcome.  The decision boundary is a \\nthreshold value that separates the classes. If the predicted probability is \\ngreater than the thre shold, it assigns the data point to one class, otherwise \\nto the other.  The most common loss function used in logistic regression is '),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 20}, page_content=\"the log -likelihood loss, which measures the difference between predicted \\nprobabilities and actual outcomes.  \\n \\n \\n                    \\n \\nThis model is well used for binary classification. So in order to avoid \\noverfitting in logistic regression we do regularization.  \\n \\n\\uf076 KNN:  \\n \\nK-Nearest Neighbors (KNN) regression is a simple and intuitive machine \\nlearning algorithm used for classificati on tasks. It's an instance -based \\nlearning method that makes predictions by considering the average (or \\nweighted average) of the target values of its k -nearest neighbors in the \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 21}, page_content='training data . During training, the KNN algorithm stores the entire training \\ndataset.  \\nTo make a prediction for a new data point, the algorithm identifies the k -\\nnearest data points (neighbors) in the training set based on a distance \\nmetric (usually Euclidean distance). K is a hyperparameter here. The \\ndistance is calculated between the entere d point from all points in the \\nwhole data set then the data set is sorted based on distance in ascending \\norder and picked the first k rows and the mode  of target variables are \\ncalculated which is the answer. The value of k is a hyperparameter that you \\nneed  to specify. A smaller k (e.g., 1 or 3) makes the model sensitive to noise \\nin the data and can result in a more variable prediction. A larger k (e.g., 10 \\nor 20) provides a smoother prediction but might not capture local patterns \\nas effectively. KNN classifi er is simple to understand and implement. It can \\ncapture complex and nonlinear relationships between features and the \\ntarget variable. Choosing the appropriate value of k is crucial and can be \\nchallenging. KNN can be computationally expensive when the dat aset is \\nlarge, as it requires calculating distances to all data points during prediction. \\nKNN classifier  works well for small to moderately -sized datasets with a \\nreasonable number of features. For large datasets, the computational cost \\nof finding nearest n eighbors can become prohibitive. If overfitting occurs in \\nKNN regression then do better feature selection, adjust the value of k, do \\nfeature selection etc.  \\n \\n\\uf076 Naïve Bayes Algorithm:  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 22}, page_content='Naive Bayes is a simple and intuitive machine learning algorithm used for \\nclassification tasks. It\\'s based on Bayes\\' theorem and assumes that the \\nfeatures used to make predictions are independent of each other (which is \\nwhy it\\'s called \"naive\").  Naive Bayes relies on Bayes\\' theorem .  \\n \\nConsider above is a data set and we have to use naïve Bayes to predict \\nwhether we can play tennis or not. So we first calculate the prior \\nprobabilities based on the given data what is the probability that yes we can \\nplay tennis and no we cannot? After that, we calculate the conditional \\nprobabilities  of all input features as shown below.  \\n                          \\n                 \\nThen using the above data we do calculations for the testing data. So we \\nhave to do a prediction for the following data.  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 23}, page_content=' \\nAs the probability of no is higher than yes we will predict that tennis cannot \\nbe played. This is an example of multinomial naïve bayes.  \\n \\nNow in the above example, all input features were discrete what if the input \\nfeatures have continuous values then we cannot calculate the conditional \\nprobability We will use Gaussian Naïve Bayes.  \\n                      \\nWe have to predict using data that the person is male or female so we will \\ncalculate the mean and standard deviation of each input feature for male \\nand female and first we calculate the prior probabiliti es. \\n                                     \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 24}, page_content='                      \\nNow we will calculate for all  \\n \\nNow we have to predict whether  a person is male or female based on the \\nfollowing data  \\n \\nFor that, we can use the Gaussian distribution equation.  \\n                                           \\nUsing Gaussian distribution we calculate the following values  \\n \\n    As the posterior probability of female is more so we will predict that \\nperson will be female. Naive Bayes is computationally efficient . It can \\nhandle a large number of features, making it suitable for high -dimensional \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 25}, page_content='data sets.  Sometimes we face an issue called zero probability issue and to \\ndeal with that we use Laplace smoothing.  \\n                                 \\nYou can see for overcast|No  the probability is zero.  \\n \\nNow probability will change from 0 to some positive value.  \\n         \\nIf overfitting occurs in naïve Bayes use feature selection, Laplace smoothing, \\netc. \\n\\uf076 Decision Tree : \\n \\nImagine you have a dataset with different types of fruits, and you want to \\nclassify them as either apples, oranges, or bananas. A decision tree \\nalgorithm helps make these classifications based on features like color, size, \\nand texture.  The goal  of decision trees, is to partition the data into subsets \\nbased on the inpu t features, leading to decisions or predictions.  There is a \\nroot node that is the first node of a decision tree also you can say it ’s a \\nnode that does not have a parent. Then there is a leaf node which is the \\nnode who do not have child nodes. The other are decision nodes that are in \\nbetween these 2 nodes . Below is the data and its corresponding tree.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 26}, page_content=' \\n \\nNow we must select the root node among the input features which one will \\nbe a root node. So first we will keep all inputs as root and will do calculation \\nlike if the root node is popcorn then if someone li kes popcorn then will it \\nloves cool as ice or not or if someone who does not love popcorn will \\nsomeone love cool as ice or not.  \\n                       \\n                           \\nYou have seen that 2 leaves of popcorn and one leaf of soda contain a \\nmixtur e of people who love as cool as ice, and some do not it is called \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 27}, page_content='impure. While the right leaf of soda does not contain the mixture it is called \\npure.  There are several ways to quantify the impurity known as entropy, \\ninformation gain, and Gini impurity. We  will calculate the Gini impurity of \\nnodes. We calculate it as below. First we calculate the Gini impurity of \\nleaves. For the left leaf:  \\n \\nFor right leaf:  \\n  \\nNow the total Gini impurity of a node is calculated as,  \\n \\nWe do the same calculations for soda. Fo r input feature age as it  contains \\nthe continuous value, we first sort it in ascending order and calculate the \\naverage of 2 adjacent age values and then calculate the Gini impurity value \\nof each average age.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 28}, page_content='                      \\nNow here is an example of  how we calculate the Gini impurity value of each \\naverage values.  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 29}, page_content=' \\n \\nIn the same way, we calculate the Gini impurity for all average values and \\nselect the minimum one. Now we see among all the gini impurity value of \\nsoda was minimal  all so we selected s oda as the root node.  \\n                          \\nNow we see the left node is n=impure so will split it in order to reduce the \\nimpurity. We will follow the same steps as explained earlier and will select \\nthe node with the minimum value. As by calculations w e observed age less \\nthan 12.5 has less gini impurity so we will select it as a node. so in the \\ndecision  tree which depth tree would  be good with a depth of 5 or 7 , we \\nbuilt different trees  to check their  accuracy and selected  the one  with the \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 30}, page_content='best accuracy and best performed  on the training set . Decision trees are \\neasy to interpret and visualize.  Can handle both categorical and numerical \\ndata.  Can capture complex relationships in data.  But they may be sensitive \\nto small variations in data and i n order to avoid overfitting in the decision \\ntree we do pruning.  \\n\\uf076 Random Forest Classifier:  \\n \\nRandom Forest is an ensemble learning method used for both classification \\nand regression tasks in machine learning.  It builds multiple decision trees \\nduring training and merges their outputs to get a more accurate and stable \\nprediction.  Each tree in a Random Forest is trained on a different random \\nsample of the training data, allowing it to learn different aspects of the \\ndata.  Randomly select a subset of data (with replacement) for training each \\ntree. Some data points may be included multiple times, while others may \\nnot be included at all  called out of the bag data set.  So using this data set \\nwe can check the accuracy of random forest.  The portion of out of ba d data \\nset that is incorrectly classified is called out of bad error. This is usually \\nknown as bootstrapping. The data  set is called a bootstrap data set. Then  \\nwe create the decision tree for each data set using above explained \\ninformation.  Random forest c an handle large data and its accuracy is better \\nthan decision tree.  But it requires more computational resources.  In order \\nto avoid overfitting do pruning ( limit the depth of a tree), etc.  \\n \\n\\uf076 Support Vector Machine:  \\n \\nSupport Vector Machines, often abbreviated as SVM, are a powerful class of \\nsupervised machine learning algorithms used for classification and \\nregression tasks. They are particularly effective when dealing with complex \\ndatasets where there is no clear linear separation between different classes \\nor groups.  The primary goal of SVM is to find the best possible boundary (or \\nhyperplane) that can effectively separate different classes in the dataset. \\nThis boundary is known as the \"decision boundary\" or \"hyperplane.\"  SVM \\naims to maximize the mar gin between the decision boundary and the \\nnearest data points from each class. These nearest data points are called '),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 31}, page_content='\"support vectors.\" Maximizing the margin not only ensures a good \\nseparation but also enhances the model\\'s generalization to unseen data.  In \\ncases where the data isn\\'t linearly separable in its original feature space, \\nSVM uses a \"kernel trick\" to map the data into a higher -dimensional space \\nwhere linear separation becomes possible. Common kernel functions \\ninclude polynomial kernels and radial b asis function (RBF) kernels.  Training \\nan SVM involves minimizing a cost function. This function tries to find the \\noptimal hyperplane that maximizes the margin while minimizing \\nclassification errors.  To classify a new data point, you apply the learned \\ndecis ion boundary. You check which side of the hyperplane it falls on.  Like in \\nan equation of decision boundary , the values are entered if its positive it\\'s \\nclassified into one class otherwise,  if negative then classified to another \\nclass.  The cost function is  \\n                                  \\nWhere c is a hyperparameter that tells how many data points can be \\nmisclassified. While the other parameter tells the distance between \\nmisclassified data points and marginal planes.  \\n \\n\\uf076 XGBoost Classifier:  \\n \\n \\nXGBoost (eXtreme Gradient Boosting) is a powerful ensemble learning \\nalgorithm known for its efficiency and high performance across a wide \\nrange of tasks, including classification . XGBoost is an ensemble learning \\nmethod that combines the predictions of multiple wea k learners (usually \\ndecision trees) to create a stronger, more accurate model. It does this by \\niteratively building trees and then combining their predictions.  We basically \\nuse the initial prediction for building an initial tree that is 0.5 means 50% \\nchanc e of happening. The data set is below,  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 32}, page_content='                        \\nNow we calculate the residual values by subtracting the training data set and the \\ninitial predicted value.  \\n                                                      \\nThe formula for the similarity  score is,  \\n            \\nNow the similarity score for root is 0 which is of residual values. Now for building \\na tree, we first have to choose the root node that has a high value of gain. For this, \\nwe took the last 2 data points and calculated it average whi ch is 15 which is the \\nthreshold value.  \\n                                    \\n \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 33}, page_content='                         \\nSo similarity score of a left node is 0.33.  \\n                                   \\nThe similarity score of the right node. Now calculate the gain the formula is  \\n                             That  is 1.33.  \\nNow check the same gain value for other threshold values  and calculate the gain.  \\nAnd by calculation dosage less than 15 has more gain so it will be root node.  \\n                                                    \\nNow as there is only one residual in right node so we will not split it but will split \\nthe left node. It has 3 residual values so select the thershold values and check \\nwhich thershold gives the high value of gain then select it. So dosage less than 5 \\nwill be selected.  \\n                                                             \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 34}, page_content='The minimum number of residuals in each leaf is determined by calculating the \\ncover. When XGBoost is used for classification  the c over is equal to  \\n                 \\nFor regression the cover equals to  \\n                   \\nIn order to avoid overfitting we do pruning we prune by calculating the difference \\nbetween gain and gamma. If the answer is positive we do not remove the node \\nbut if the difference is negative we prune the node.  Now for classification the \\noutput value for leaves are :  \\n         \\n                    \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 35}, page_content='                              \\nSo for making a prediction, we use the initial prediction value.  We need to \\nconvert the probability to log(odds) value.  \\n                                        \\n                                                                       \\nThe value of log(odds) is 0. So the predictions are made using,  \\n                      0.3 is \\nlearning rate.  \\nTo convert the log(odds) value into probability we plug it into the logistic function.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 36}, page_content='                                                      \\n                                               \\n                                                       \\nThe residual value gets shorter now. Now we built the secind tree using new \\nresiduals and so on. For the  second tree the residuals are  \\n                                                              \\nThe similarity score will look like,  \\n        \\n \\nIt is known for its high accuracy.  \\n \\n \\n \\n \\n \\n \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 37}, page_content='Un-Supervised Learning : \\nUnsupervised learning is a type of machine learning where the algorithm learns \\nfrom unlabeled data. Unlike supervised learning, there are no target labels \\nprovided. Instead, the algorithm identifies patterns, relationships, and structures \\nin the data on its own.  \\nSome examples of unsupervised learni ng are anomaly detection, pattern \\nrecognition, audience segmentation, etc.  \\n \\nTypes of unsupervised learning tasks:  \\n \\n1. Clustering  \\n2. association  \\nClustering : \\nUnsupervised clustering is a type of machine learning where the algorithm tries to \\nfind natural groupings or clusters within a dataset without being provided with \\nany specific labels or target information.  \\nExamples of clustering include image segmentation, anomaly detection, etc.  \\nAlgorithms:  \\n\\uf076 K-means : \\n \\nK-means is a centroid -based clustering algorithm, where we calculate the \\ndistance between each data point and a centroid to assign it to a cluster. \\nThe goal is to identify the K number of groups in the dataset.  It is an \\niterative process of assigning each data point to the groups and slowly data \\npoints get clustered based on similar features.  Here, we divide a data space \\ninto K clusters and assign a mean value to each. The data points are placed \\nin the clusters closest to the mean value of that cluster.  Let consider the \\ndata we have given below,  '),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 38}, page_content='         \\n \\nThe first step is to define the K number of clusters in which we will group \\nthe data. Let ’s select K=3.  The centroid  is the center of a cluster but initially, \\nthe exact center of data points will be unknown so, we select random data \\npoints and define them a s centroids for each cluster. We will initialize 3 \\ncentroids in the dataset.  \\n                   \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 39}, page_content='Now that centroids are initialized, the next step is to assign data points Xn \\nto their closest cluster centroid Ck  this is done by calculating the Euclidean \\ndistance between the data set and centroid and assigning a data point to a \\ncluster where the distance between data point and a particular cluster ’s \\ncentroid is less.  \\n                   \\nNext, we will re -initialize the centroids by calculating the average of all data \\npoints of that cluster.  \\n                   \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 40}, page_content='We will keep repeating steps 3 and 4 ( finding the distance between data \\npoints and centroids and after that reinitializing the centroids) until we have \\noptimal centroids and the assignments of data poin ts to correct clusters are \\nnot changing anymore.  \\n                       \\nHere we clustered the data set. Now for initializing the value of k, there are \\nmany ways we can also select the k centroids randomly or we can use the \\nmost appropriate method k -means ++. K-means++ is a smart centroid \\ninitialization method for the K -mean algorithm . There are simple steps first \\nrandomly pick the first centroid then calculate the distance between all data \\npoints and the selected centroid , and select the one with the farth est \\ndistance from the first centroid . Repeat the steps until you get the k \\ncentroids.  \\nNow discuss how to know the value of k for that we use the famous elbow \\nmethod. We select a range of k values like 1,2,3,4 and so on. Then find the \\ndistance between data points and the centroid and find the squared sum \\ndistance. Create a plot where the x -axis represents the number of clusters \\n(K) and the y -axis represents the corresponding SSD values.  Repeat the \\nsteps for other values of k and then select the elbow point.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 41}, page_content='             \\nThe distortion (or inertia) in k -means clustering measures how spread out \\nthe data points are within each cluster. It\\'s calculated as the sum of the \\nsquared distances between each data point and its corresponding cluster \\ncentroid.  As you incr ease the number of clusters (k), the distortion tends to \\ndecrease. This is because as you add more clusters, each data point tends to \\nbe closer to its nearest centroid. So, initially, adding more clusters leads to a \\nsignificant reduction in distortion.  How ever, after a certain point, adding \\nmore clusters doesn\\'t result in a significant reduction in distortion.  The \\n\"elbow point\" is where the distortion starts to flatten out. It\\'s the point \\nwhere the rate of decrease sharply changes, forming an \"elbow\" shape in \\nthe distortion vs. k plot.  \\nK-Means is easy to understand and implement, making it a quick and \\nefficient clustering method.  It can handle large datasets efficiently, making it \\nsuitable for big data applications.  K-Means is computationally faster \\ncompared  to other clustering algorithms, making it suitable for real -time \\napplications.  But, You need to specify the number of clusters in advance, \\nwhich can be challenging in some cases.  Outliers can significantly impact \\nthe clustering results, potentially leadin g to inaccurate cluster assignments.  \\nHere are some steps you can take to address overfitting in K -Means :  \\nReduce the number of clusters(k), address outliers , etc. \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 42}, page_content='\\uf076 Hierarchal  Clustering:  \\nHierarchical clustering is a method used to group together similar dat a \\npoints based on their features. It creates a tree -like diagram called a \\ndendrogram, which shows the arrangement of clusters.  \\nHierarchal clustering is of two types:  \\n\\uf0b7 Agglomerative clustering  \\n\\uf0b7 Divisive clustering  \\nIn Agglomerative Hierarchical Clustering each  data or observation is treated \\nas its cluster. A pair of clusters are combined until all clusters are merged \\ninto one big cluster that contains all the data.  \\nIn Divisive Hierarchical Clustering , entire data or observation is assigned to a \\nsingle cluster. The cluster is further split until there is one cluster for each \\ndata or observation.  \\nSo, both are the reverse of each other here I will explain agglomerative \\nclustering.  \\nSo, in agglomerative clustering for each point initially , we will consider it as \\na separate cluster. Next, we will find the nearest point and create a new \\ncluster. So, we keep on repeating the steps until we get a single cluster.  \\n                                   \\n \\n \\nNow to check how many clusters are there we u se a dendogram.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 43}, page_content='                                    \\nLike the first clusters of P1 and P2 are formed the P4 and P5 and we keep \\ncontinuing steps to check for the nearest points and form clusters now the \\nthing is what will be the value of k ? For we take a th reshold value and for a \\nthreshold , we take the longest vertical line where no horizontal line passes  \\nthrough it. Like threshold may be of distance 5,3 or 1.  \\n                              \\nSo the threshold found at a distance equals 5 and will draw a horizontal line \\nand will check how many points line has cut in the above case 2 points are \\nformed by cutting the vertical line so value of k will be 2. Here is how we \\nchoose threshold value.  \\nSo, divisive clustering starts with all data points in a single c luster and then \\nrecursively divides them into smaller clusters.  In divisive clustering, we start \\nwith all data points belonging to one large cluster.  We look for the cluster \\nthat is the least cohesive (i.e., it contains data points that are less similar to  \\neach other). This cluster is then split into two smaller clusters.  This process \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 44}, page_content=\"of identifying the least cohesive cluster and splitting it continues recursively \\nuntil each data point is in its own cluster .  Similar to agglomerative \\nclustering, you can crea te a dendrogram to visualize the process.  To decide \\non the number of clusters, you can use a similar approach as in \\nagglomerative clustering. Look for the threshold where you get the desired \\nnumber of clusters.   \\nThe algorithm is used well if the data set i s small if we have a large data set \\nthen k means will be good.  Hierarchical clustering is less sensitive to outliers \\ncompared to K -means.  Hierarchical clustering can be more computationally \\ndemanding, especially for large datasets  \\n \\nAnomaly Detection:  \\nAnoma ly detection, also known as outlier detection, is the process of identifying \\ndata points or patterns that deviate significantly from the norm in a dataset. These \\ndeviations are often indicative of unusual or unexpected behavior, which may \\nwarrant further i nvestigation.  Anomaly detection is widely used across various \\ndomains, including cybersecurity, fraud detection, healthcare, manufacturing, \\nfinance, and more. It helps identify rare events or irregularities that may have \\nsignificant implications.  In a Gaus sian (Normal) Distribution, the probability \\ndensity is highest at the mean and decreases symmetrically as you move away \\nfrom the mean in both directions. This is what creates the characteristic bell -\\nshaped curve.  \\nAlgorithm:  \\n\\uf0b7 Choose n features xi that you think might be indicative of anomalous \\nexamples  (Create  a histogram for the feature. This shows the distribution of \\nvalues. In a Gaussian Distribution, you'll see a bell -shaped curve.  Does it \\nresemble a bell curve? If so, it's an indicator that the data mi ght follow a \\nGaussian Distribution.  If the curve is heavily skewed, the data might not \\nfollow a Gaussian Distribution.  So, you can apply mathematical \\ntransformations to the data to make it more Gaussian -like.) \"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 45}, page_content=\"     \\n \\n\\uf0b7 Find the mean and variance of each inpu t feature.  \\n \\n\\uf0b7 Given new example x compute p(x),  \\n          \\n\\uf0b7 Anomaly if p(x)<epsilon ( a threshold value)  \\nNow anomaly detection can be used over supervised learning in many cases like, \\nWhen you have limited or no labeled data for anomalies.  When you don't hav e a \\nclear understanding of all possible types of anomalies. Anomaly detection can \\ndiscover unknown anomalies without prior knowledge.  \\n \\n \\nRecommendation Systems : \\nRecommendation systems are intelligent algorithms that assist users in \\ndiscovering relevant and personalized content, products, or services. They play a \\nvital role in enhancing user experiences across various platforms, from e -\\ncommerce websites to streaming services.  Recommendation systems have \\nrevolutionized the way users interact with platforms and  services. They not only \\nimprove user engagement but also drive sales, increase customer satisfaction, and \\nfoster loyalty.   \\nExample: Amazon items recommendation, Netflix movie recommendation.  \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 46}, page_content=\"There are 2 common techniques used in recommendation systems ex plained \\nbelow.  \\nTechniques : \\n\\uf076 Collaborative Filtering:  \\n \\n“Linear regression is used to predict the ratings that a user might give to a \\nmovie they haven't seen yet. The idea is to find a linear relationship \\nbetween the features (such as user behavior, movie cha racteristics, etc.) and \\nthe ratings. This allows us to estimate what rating a user might give to a \\nmovie based on their historical preferences and characteristics of the \\nmovie. ” \\n \\n“Logistic regression, on the other hand, is used in a different part of the \\nrecommendation system. It can be used to answer binary questions like Did \\nuser j watch the movie I after being shown? This involves a yes/no \\nprediction, which is a classification problem.  Logistic regression is well -\\nsuited for this because it models the probability of a binary outcome.  In a \\nrecommendation system, this could be used to predict whether a user will \\nwatch a recommended movie or not based on their behavior and \\npreferences. ” \\n \\nFor working on movie recommendations,  the model is the same as the \\nlinear regression:  \\n \\n \\n \\nWe have movie and user matrix data and entries are rated  and there are \\nmany missing values so our goal is to build  a model that predicts it well.  \\n \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 47}, page_content='       \\n \\nso, for user and item parameters are trained and cost function is reduced \\nusing  gradient descent , and the correct value of parameters is obtained . \\nFirst of all , we will calculate the parameters of movies and after that , we will \\ncalculate for parameters of users  to predict the ratings for missing values for \\nchoosing the number of para meters of movies we use cross -validation  to \\nuse different values and then select which gives high accuracy.   \\nThe cost to learn parameters for user j is,  \\n \\n \\nTo learn parameters for all users the overall cost function is below,  \\n \\nNu=> number of users  \\n \\nNow to learn the parameters for the movie the cost function is,  \\n \\n \\n \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 48}, page_content=' \\nTo learn the parameters for all movies the overall cost function is,  \\n \\n \\n \\nnm=> total number of movies.  \\n \\nNow, the overall cost function to learn the parameters for users and movies \\nis give n below,  \\n \\n \\n \\nTo reduce the cost function and get the best parameters we may use \\ngradient descent.  \\n                 \\n \\nNow using the model we fill in the missing values so we recommend the \\nmovies user j has liked the sci-fiction  movies so we recommend other sci-\\nfiction  movies highly rated by other people to user j predicting  that user j \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 49}, page_content=\"may like this . In collaborative filtering, you don't have to explicitly choose \\nwhether to find the best parameters for users or movies first. The process \\ninvolves updating bot h sets of parameters (users and movies) in an iterative \\nmanner.  \\n \\nHere's a simplified step -by-step process:  \\n \\nStart with initial parameter values for both users and movies.  \\nUpdate movie parameters while keeping user parameters fixed.  \\nThen, update user parameters while keeping movie parameters fixed.  \\nRepeat steps 2 and 3 in an iterative manner.  \\nContinue this process until the model converges (meaning the cost function \\nstabilizes or changes very slowly).  \\nThis iterative process allows the model to graduall y learn the best \\nparameters for both users and movies simultaneously. So, there's no \\nspecific order to follow; you update both sets of parameters in a back -and-\\nforth manner until the model learns the relationships between users and \\nmovies.  \\n \\nNow for binary classification cases used in recommendation system, the \\nequations are below,  \\n \\n \\n \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 50}, page_content=' \\n \\n \\n \\n\\uf076 Content -based  Filtering:  \\n \\n \\n \\nContent -based filtering is another important technique used in \\nrecommendation systems. Unlike collaborative filtering, which relies on \\nuser -item interactions and similarities, content -based filtering focuses on \\nthe attributes or features of items and users . \\nFor example if we are working on a movie recommendation system,  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 51}, page_content=' \\n \\n \\n \\nFor finding these vectors we will use neural network.  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 52}, page_content=\" \\n \\nThe cost function will be,  \\n \\n \\n \\nIf the dot product between a user's preference vector and the feature \\nvector of a sci -fi movie is large, it indicates a high level of alignment or \\nsimilarity. This suggests that the user's preferences are well -matched with \\nthe f eatures associated with sci -fi movies.  \\nIf the distance above is small then it means movies are similar if we find the \\ndistance between the vectors of Interstellar  and The Martian  the distance \\nwill be small.  So if a user likes science fiction movies such movies will be \\nrecommended to users which are more similar to science fiction.  \\n \\n \\nSummary:  \\n \\nBoth collaborative filtering and content -based filtering have their strengths \\nand weaknesses. The choice between them  depends on the specific context \\nand requirements of the recommendation system.  \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 53}, page_content=\" \\nCollaborative filtering doesn't require explicit feature extraction. It learns \\npreferences directly from user -item interactions, making it suitable for a \\nwide range of domains . It requires a sufficient amount of user -item \\ninteraction data to make accurate recommendations, which can be an issue \\nin sparse datasets.  Content -based filtering offers clear interpretability, as \\nrecommendations are based on explicit item features (e.g.,  genre, director).  \\nIt requires explicit feature extraction and engineering, which can be time -\\nconsuming and may require domain knowledge.  \\nIf you have a large dataset with significant user -item interactions, \\ncollaborative filtering can be very effective.  When you have detailed and \\nexplicit features for items that can be used for recommendation (e.g., \\ngenre, tags, attributes)  content -based filtering will be best.  \\nCombining collaborative and content -based filtering can often yield the best \\nof both worlds. Hyb rid models can mitigate the limitations of individual \\nmethods.  \\n \\nDimensionality Reduction:  \\nTechniques : \\n\\uf076 PCA ( Principal  Component Analysis):  \\n \\n“Process of figuring out the most important features that has the most \\nimpact on the target variable. ” \\n \\nPrincipal Component Analysis (PCA) is a technique used in data analysis and \\ndimensionality reduction. It helps find the underlying patterns in complex \\ndata by transforming it into a new coordinate system where the data's \\nvariability is maximized along the new axes, called principal components. \\nThis makes it easier to visualize and analyze the data while retaining as \\nmuch relevant information as possible . \\nWhen you have a high -dimensional dataset with many features (like 10), \\nvisualizing it directly becomes challenging  because our visual perception is \"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 54}, page_content='limited to three dimensions at most.  PCA helps by transforming the data \\ninto a lower -dimensional space (like 2D or 3D) while retaining as much of \\nthe important information as possible. This makes it much easier to \\nvisualiz e and understand the relationships between data points.  \\n \\n \\n \\n \\nBefore applying PCA first you should do feature scaling.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 55}, page_content=' \\n \\n \\n \\nConsider the data ,  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 56}, page_content='                          \\n \\n \\nBest choice,  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 57}, page_content=' \\n \\n \\nNow lets understand the PCA.  \\nThe data set is below,  \\n                    \\nVisualization of data.  \\n \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 58}, page_content='             \\n \\nFirst  we plot the data then we calculate the average measurement of Gen1 \\nthen for Gen 2.  \\n                 \\nNow we will shift the data so that the center is on the top of origin.  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 59}, page_content=\" \\nNow we will tr y to fit the line to it. We will draw ing  a random line that goes \\nthrough the origin then we rotate the line until it fits the data well.  \\n \\n \\nNow the question  is how PCA knows which line is fit or not let's look below.  \\nSo, first , we project the data onto the line then  for the best -fit line, either we \\nminimize the distance between  the data points and the line or we try to find the \\nline that maximizes the distances from projected points to the origin.  \\nNow lets look into what PCA doe s to a single point.  \\n\"),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 60}, page_content='                  \\nSo you can see that the distance between the point to the origin remains the \\nsame whether the line moves or not.  \\n                         \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 61}, page_content='Now if the line fits best in the data then the b length will decrease and t he c \\nlength will increase.        \\n \\nSo PCA fits the best line by maximizing the sum of squared distances from \\nprojected points to the origin. This line is called the principal component . \\nThis principal component is a linear combination of all input features . \\nNow consider if you have 100 features and you cannot visualize it as it ’s 100D. So \\nyou will reduce it to 2D so that you can visualize the data easily. Then you will \\ndraw a line called PC1 as explained above.  The goal is to find a line that captures \\nthe most variation in the data  means the majority of data points lie near the PC1.  \\nThen you will plot PC2 exactly perpendicular to PC1 without further optimization.  \\nNow, you have two lines (PC1 and PC2) that represent the most important \\ndirections of variatio n in your data. Together, they form a new coordinate system.  \\nInstead of working with the original 100 parameters, you can now use these two \\ncomponents to represent your data points in a lower -dimensional space.  \\nFor example, if the equations for PC1 and PC2  are: \\nPC1:   0.7 x1 + 0.3 x2  \\nPC2:   -0.3 x1 + 0.7 x2  \\nThen, for a given data point with original feature values  x1=3 and x2=4  you can \\ncalculate the corresponding coordinates  along PC1 and PC2.  \\nX* = 2.9  \\nY*= 2.5  \\nSo, x* and y* will be the new coordinates of the data point. You can then plot \\nthese transformed points to visualize the data in terms of the principal \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 62}, page_content='components.  PCA assumes linear relationships and may not always capture \\ncomplex, non -linear patterns in the data.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nMore things coming soon !  \\n \\nPrepared by Muhammad Rayyan  \\n \\nLinkedin : Linkedin  \\nEmail: m18rayyan@gmail.com  \\nMedium : Medium  \\n'),\n",
       " Document(metadata={'source': 'ML_NoteBook.pdf', 'page': 63}, page_content=' ')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDF file \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"ML_NoteBook.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bffd58b7-558c-4a4a-a19a-3b273ac397d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "252586ab-b94c-4502-94f1-8dfd79ac7da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10ec722e-6d5f-418b-8360-7d4a8c5e9bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_paths=(\"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a68eabe-ccef-491c-975e-07cdf61a268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_path=(\"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\",),\n",
    "                      )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ead9d52-05c8-4caf-be09-57a23d874959",
   "metadata": {},
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e49cc67-56d0-4969-ac99-394c37287158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a632c2eb-75a0-4da1-893b-b72664bf14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_path=(\"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"mw-page-title-main\", \"noprint\", \"mw-file-description\")\n",
    "                       ))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f34d432f-f263-44a4-b794-2e0a92deb1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need'}, page_content='Attention Is All You NeedFrom Wikipedia, the free encyclopedia\\n\\n\\n')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a11c3cf-c8b8-4c45-9e2d-afb320c9244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=4956557ba7be891e7d8a3c5a7553da326f7b75b12c16301ab68bbb9ce9942849\n",
      "  Stored in directory: c:\\users\\yuvraj pandey\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "976361d2-c99b-4345-b3e0-8f3ca9ebf996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.12-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.12-cp39-abi3-win_amd64.whl (16.0 MB)\n",
      "   ---------------------------------------- 0.0/16.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.0 MB 4.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.0/16.0 MB 4.6 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.3/16.0 MB 2.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.6/16.0 MB 3.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.2/16.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.5/16.0 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.6/16.0 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.6/16.0 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.7/16.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.7/16.0 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.5/16.0 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.0/16.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.8/16.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/16.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/16.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.7/16.0 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/16.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.0/16.0 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbc66046-7d9b-4474-9138-d249b1e89288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Direct access and communicate all the research paper on arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"2410.18897\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d82671c3-bf8a-4170-b543-4476c5f44892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-10-24', 'Title': 'Generation of synthetic financial time series by diffusion models', 'Authors': 'Tomonori Takahashi, Takayuki Mizuno', 'Summary': 'Despite its practical significance, generating realistic synthetic financial\\ntime series is challenging due to statistical properties known as stylized\\nfacts, such as fat tails, volatility clustering, and seasonality patterns.\\nVarious generative models, including generative adversarial networks (GANs) and\\nvariational autoencoders (VAEs), have been employed to address this challenge,\\nalthough no model yet satisfies all the stylized facts. We alternatively\\npropose utilizing diffusion models, specifically denoising diffusion\\nprobabilistic models (DDPMs), to generate synthetic financial time series. This\\napproach employs wavelet transformation to convert multiple time series (into\\nimages), such as stock prices, trading volumes, and spreads. Given these\\nconverted images, the model gains the ability to generate images that can be\\ntransformed back into realistic time series by inverse wavelet transformation.\\nWe demonstrate that our proposed approach satisfies stylized facts.'}, page_content='Generation of synthetic ﬁnancial time series by\\ndiﬀusion models\\nTomonori Takahashi∗† and Takayuki Mizuno‡\\n† The Graduate University for Advanced Studies, SOKENDAI, 2-1-2 Hitotsubashi, Chiyoda-ku,\\nTokyo, Japan\\n‡ National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan\\nAbstract\\nDespite its practical signiﬁcance, generating realistic synthetic ﬁnancial time series is challenging due\\nto statistical properties known as stylized facts, such as fat tails, volatility clustering, and seasonality\\npatterns. Various generative models, including generative adversarial networks (GANs) and variational\\nautoencoders (VAEs), have been employed to address this challenge, although no model yet satisﬁes all\\nthe stylized facts. We alternatively propose utilizing diﬀusion models, speciﬁcally denoising diﬀusion\\nprobabilistic models (DDPMs), to generate synthetic ﬁnancial time series. This approach employs\\nwavelet transformation to convert multiple time series (into images), such as stock prices, trading\\nvolumes, and spreads. Given these converted images, the model gains the ability to generate images that\\ncan be transformed back into realistic time series by inverse wavelet transformation. We demonstrate\\nthat our proposed approach satisﬁes stylized facts.\\n1.\\nIntroduction\\nIn ﬁnancial markets, many time series data like stock price ﬂuctuations are generated and recorded\\nevery day. Financial time series are of great interest to practitioners and theoreticians for pre-\\ndictions. Because of the enormous amount of data in ﬁnancial time series and their similarity to\\nphysical systems, such as being composed of a large number of agents interacting with ﬁnancial\\nmarket participants, ﬁnancial time series have become a ﬁeld of study not only in economics but\\nalso in statistics and physics (Plerou et al. 2000, Chakraborti et al. 2007).\\nIt is known empirically that even for such processes as stock prices, which are expected to follow\\nBrownian motion (Bachelier 1900), the variates follow a fat-tailed distribution rather than a normal\\ndistribution. Studies have demonstrated that this distribution obeys a power law and explored the\\nmechanisms by which this power law arises (Gabaix et al. 2003, Gabaix 2009). Additionally, studies\\nhave been conducted on volatility, which represents the magnitude of stock price ﬂuctuations.\\nAttempts have reproduced volatility clustering, which involves persistent periods of high or low\\nvolatility, as well as the autocorrelation of the time series of volatilities (Chakraborti et al. 2007,\\nCont 2001, Ratliﬀ-Crain et al. 2023, Bollerslev et al. 1992, Lux 2009, Samanidou et al. 2007).\\nThese properties of ﬁnancial time series, such as fat-tailed distribution and volatility clustering,\\nare called stylized facts. These attempts utilized parametric models, such as the ARCH model and\\nagent-based models, although the former’s goal is to represent the mechanism from which stylized\\n∗Corresponding author. Email: t takahashi@nii.ac.jp\\narXiv:2410.18897v1  [q-fin.CP]  24 Oct 2024\\nfacts emerge, not to reproduce plausible ﬁnancial time series. Through these studies, the existence\\nand universality of stylized facts have been disseminated among researchers.\\nAnother workstream for ﬁnancial time series is the generation of synthetic time series. Recent\\nremarkable developments in machine learning technology have led to the emergence of models that\\ngenerate images and natural language texts that resemble those made by human hands. These\\nnovel technologies have been applied to the generation of synthetic ﬁnancial time series data, and\\nsome studies suggest the reproduction of realistic ﬁnancial time series (Brophy et al. 2023, Eckerli\\nand Osterrieder 2021, Wiese et al. 2020, Dogariu et al. 2022). These studies focused on generating\\nrealistic ﬁnancial time series in terms of reproducing stylized facts. However, despite many eﬀorts,\\nno model has successfully reproduced every stylized fact (Dogariu et al. 2022).\\nWe address the limitations of the current synthetic ﬁnancial time series generation methods and\\npropose an approach that employs denoising diﬀusion probabilistic models (DDPMs) suggested in\\nHo et al. (2020). Recognized for their excellent image generation, DDPMs surpass generative ad-\\nversarial networks (GANs) and variational autoencoders (VAEs) in producing high-quality, diverse\\nsynthetic data (Xiao et al. 2022). Our methodology uniquely incorporates wavelet transforma-\\ntion to convert ﬁnancial time series into full-color spectrogram images (Ramsey et al. 1995), en-\\nabling DDPMs to learn the intricate characteristics of these series through image analysis. Through\\npost-training, these spectrogram images are transformed back into ﬁnancial time series data. This\\nmethod leverages the trichannel nature of color images (RGB) to simultaneously train on and infer\\nthree interrelated time series, stock prices, spreads, and trading volumes, all of which are observed\\nwithin identical periods. We demonstrate that our approach captures these three time series in uni-\\nson and also accurately reproduces the stylized facts associated with them, marking a signiﬁcant\\nadvancement in the ﬁeld of ﬁnancial time series synthesis.\\nThe remainder of the article is organized as follows. In Section 2 we discuss the existing ap-\\nproaches in the literature to the stylized facts of ﬁnancial time series in econophysics and informat-\\nics. Our proposed methodology is presented in Section 3, and Section 4 discusses its experimental\\nsetup and results. Finally Section 5 discusses our results and future work.\\n2.\\nRelated works\\nTime series data, a critical component for prediction problems, has received considerable attention,\\nespecially from numerous studies aimed at enhancing prediction accuracy (Das et al. 2023). Time\\nseries data have attracted studies in both econophysics and informatics. The former focuses on the\\nstylized facts of ﬁnancial time series; the latter informatics concentrates on generating time series\\ndata. In this section, we review related works in these ﬁelds.\\n2.1.\\nRelated works in econophysics\\nFinancial time series are generated in actual ﬁnancial markets every day, every minute, or even\\nmore frequently by the activities of the participants in such markets. The most typical example of\\na ﬁnancial time series is the price movement of stocks. The spreads and trading volumes of stocks\\nare also observed in ﬁnancial markets. Here, there are two types of prices: bids and asks. The bid\\nprice refers to the buyer’s interest price, and the ask price refers to the seller’s interest price. If a\\nmarket participant wishes to immediately purchase (resp. sell) a particular stock, she can place a\\nrequest called a market order. As a result, the transaction is executed at the ask (resp. bid) price.\\nThe diﬀerence of ask and bid prices (generally, bid < ask) is called a bid-ask spread or just the\\nspread. When a transaction is executed by matching a buy market order to the ask price or a sell\\nmarket order to the bid price, the amount of the transaction’s stock is also recorded as time series\\ndata, and these amounts are called the trading volumes. The prices and spreads are time series\\ncreated by observing their snapshots at predetermined intervals: daily, hourly, minute-by-minute,\\nor even more frequently. The trading volumes are also time series based on summing the amounts\\nFigure 1. Examples of typical ﬁnancial time series. Data: minute-based\\nstock prices and trading volumes of AAPL.O on NASDAQ on January 23,\\n2014 from 10:30 to 11:30 AM EST. In a stock exchange, both bid and ask\\nprices are observed. A bid (resp. ask) price is an amount quoted by market\\nparticipants who want to buy (resp. sell) the stock. Arithmetic average price\\nof bid and ask prices for each time are called the mid-price (the mid), and\\ndiﬀerence between ask and bid prices is called the bid-ask spread (the spread).\\ntraded in the intervals. We treat these three typical time series data as ﬁnancial time series (Fig 1).\\nRegarding stock prices Si, their log returns log\\nSi\\nSi−1 are often used instead of prices to normalize\\nthe diﬀerences among various stocks. This transformation also moves stock price ﬂuctuations closer\\nto a stationary state, similar to how a random walk, which is a non-stationary time series, often\\nshows stationarity when a diﬀerence series is taken.\\nThese ﬁnancial time series, such as stock prices, resemble Brownian motions. In the early days of\\ntheoretical studies of ﬁnancial time series, modeling was done using Brownian motions (Bachelier\\n1900). Over the past few decades, many studies have proved that ﬁnancial time series exhibit unique\\ncharacteristics that are not seen in Brownian motions. As explained above, the deviation of ﬁnancial\\ntime series from Brownian motion (called stylized facts) has been studied extensively. One typical\\nstylized fact is the fat-tail phenomenon, where ﬁnancial time series tend to have heavier tails than a\\nnormal distribution, and they follow power laws (Gabaix et al. 2003, Gabaix 2009). The emergence\\nof these larger than expected movements under a normal distribution is often attributed to the\\ntrades of large participants (Gabaix et al. 2003). Another stylized fact is volatility clustering, where\\nlarge ﬂuctuations tend to be followed by periods of similarly large ﬂuctuations. This characteristic\\nappears as the slow decay of autocorrelation of volatility time series with respect to lag. Other\\nslow decays of autocorrelations are observed in time series of spreads and trading volumes, which\\ninterrelate with volatilities. This characteristic, called the long memory of time series, contrasts\\nwith the short memory of log returns of stock prices, which have the fast decay of autocorrelation.\\nSuch stylized facts are observed not only in stock markets but also in other various ﬁnancial markets\\nlike foreign exchange markets (Mizuno et al. 2003). Studies have employed parametric models to\\nrepresent stylized facts. The ARCH model is a key instrument for empirical studies (Bollerslev et\\nal. 1992), and some studies have modiﬁed random walks (Takayasu et al. 2010). Another approach\\nfor studying stylized facts is agent-based models, which simulate market participants as agents\\n(Lux 2009, Samanidou et al. 2007).\\nAdditionally, observing the movement of ﬁnancial time series throughout the day reveals intraday\\nseasonality. Volatility, spread, and trading volume are high immediately after the market opens,\\ndecrease during mid-day, and rise again as the market approaches closing. These characteristics\\nhave been studied in the context of high-frequency market microstructure (Shakeel and Srivastava\\n2018).\\nWavelet analysis is another approach for analyzing ﬁnancial time series (Ramsey et al. 1995).\\nWavelet transformation can decompose one-dimensional time series data into two-dimensional pic-\\ntures in space and time, allowing the nature of the original time series to appear in the pictures.\\n2.2.\\nRelated works in informatics\\nAttempts have explored the nature of time series and the mechanisms that cause them as well\\nas the generation of realistic synthetic time series. For instance, in the medical and social science\\nﬁelds, synthetic time series can support model training through data augmentation while preserving\\nanonymity (Brophy et al. 2023). In the ﬁnancial sector, there is a practical interest in employing\\nsynthetic time series for stress tests under hypothetical market conditions for improving the ro-\\nbustness of risk management and trading models. Given the restricted access to high-frequency\\nﬁnancial data and the scarcity of time series that demonstrate such unique phenomena as ’ﬂash\\ncrashes,’ a concerted eﬀort is focusing on generating diverse, realistic ﬁnancial time series (Eckerli\\nand Osterrieder 2021, Wiese et al. 2020, Dogariu et al. 2022).\\nGenerative adversarial networks (GANs) have been employed to produce not only ﬁnancial time\\nseries but also other general time series. GANs were originally proposed to generate images (Good-\\nfellow et al. 2014). A GAN has two types of neural network structures: a generator and a discrimi-\\nnator. The generator creates realistic data from random noise data. The discriminator receives real\\ndata or data from the generator and decides whether they are real or generated. If the discrimi-\\nnator’s decision is correct, the generator incurs a loss; otherwise, the discriminator suﬀers a loss.\\nBy continuing this process as neural network training, the generator eventually learns to generate\\nrealistic data. TimeGAN (Yoon et al. 2019) is a GAN model speciﬁcally designed to generate time\\nseries data. It introduces latent space and two functions (embedding and recovery) to map the\\ncharacteristics of input data into the latent space. As another application of GANs for time series,\\nWaveGAN (Donahue et al. 2019) trains GANs by spectrogram images converted from audio data as\\ntime series for synthetic audio data generation. For ﬁnancial time series, QuantGAN (Wiese et al.\\n2020) employs temporal convolutional networks (TCNs) for both its generator and discriminator.\\nAs another approach, variational autoencoders (VAEs) (Kingma and Welling 2014) are employed\\nfor ﬁnancial time series generation (Dogariu et al. 2022). A VAE model has latent space and two\\nneural networks, an encoder, and a decoder. The encoder is trained to embed given real data into\\nlatent space through the parameters of parametric models, and the decoder is trained to repro-\\nduce the given real data. Despite attempts using these generative models like GANs and VAEs to\\nreplicate ﬁnancial time series, no model has yet fully captured all the stylized facts (Dogariu et al.\\n2022).\\nThe characteristics denoted as stylized facts complicate the generation of synthetic ﬁnancial\\ntime series. The related works in this area have generally focused on the replication of stylized\\nfacts (Chakraborti et al. 2007, Cont 2001, Ratliﬀ-Crain et al. 2023), including fat tails, volatility\\nclustering, seasonality, and calendar eﬀects. All are absent in Brownian motions. As far as we know,\\nthe ﬁnancial time series focused on in the related works are limited to stock prices, while other\\ntime series observed simultaneously with stock prices are out of scope.\\nIn addition to GANs and VAEs, diﬀusion models are often used to generate images. The critical\\ncharacteristics of generative models for images are quality, diversity, and the generation speed. The\\nGAN, VAEs, and diﬀusion models satisfy two of these characteristics, although the third remains\\nchallenging. GANs oﬀer quality and speed beneﬁts, VAEs provide strength in diversity and speed,\\nand diﬀusion models have quality and diversity advantages (Xiao et al. 2022). For applications to\\ntime series data, diﬀusion models are used for time series imputation (Tashiro et al. 2021).\\n3.\\nMethodology\\nAs discussed above, building a generative model to generate realistic ﬁnancial time series remains\\na challenging task. Our approach for generating synthetic time series data is to utilize a diﬀusion\\nmodel: the denoising diﬀusion probabilistic model (DDPM suggested in Ho et al. (2020)). To\\nleverage its advantages (excellent image generation), we convert the time series data into images\\nto train the model. After training, the model generates images, which we then convert back into\\nthe original time series data. This approach utilizes the model’s image processing capabilities to\\nhandle time series data in a novel and potentially more insightful way. This approach involves the\\nfollowing steps. In the subsequent description, we assume the simultaneous generation of three\\nsynchronized observed ﬁnancial time series: prices, bid/ask spreads, and trading volumes.\\nPreprocessing of ﬁnancial time series: Our methodology for transforming ﬁnancial time\\nseries into synthetic data unfolds through a series of designed steps. Initially, because sequence\\ndata with size 2n simplify the subsequent discrete wavelet transformation, we expand the time\\nseries by mirror expansions at both ends to align the length of the time series to 2n. Then we\\ncalculate the log returns of the stock prices by determining the diﬀerences of the natural logarithm\\nof consecutive stock prices. This step, which is crucial for addressing the non-stationarity inherent in\\nstock price time series, renders them more amenable to analysis. Concurrently, we apply the arsinh\\ntransformation to the trading volumes to approximate a logarithmic scale for large values by and\\nto facilitate eﬀective scale transformation. Unlike the natural logarithm, the arsinh function allows\\nsmall values near 0 and 0 to enter (Bellemare and Wichman 2020). Following this, we undertake\\na power transformation on the series and normalize it as (Xt−µ(Xt))\\n1\\np\\nσ(Xt)\\n. The power index can be\\ndiﬀerent for each dimension in multivariate time series. In the steps so far, time series Xt contains\\nmany outliers, which reduce the training and inference eﬃciency. We substitute such outliers with\\nthe given z-values of the normalized data through winsorization. For example, if Xt > z (resp.\\nXt < −z), then such Xt are substituted by z (resp. −z).\\nWavelet transformation and ﬁlling of pixels of an image: After applying discrete wavelet\\ntransformation to the preprocessed time series, we obtain sequences of wavelet coeﬃcients. For\\nthe transformation, we employed the Haar wavelet as the mother wavelet. Because of the mirror\\nexpansion for making the length of the time series 2n, we obtain a zero-th order coeﬃcient, a\\nﬁrst order coeﬃcient, two second order coeﬃcients, four third order coeﬃcients, and so on up to\\n(n −1)-th order coeﬃcients. These coeﬃcients are regarded as luminance and are arranged to\\nﬁll the pixels, transforming each time series into a monochrome image. In this pixel ﬁlling, k-th\\norder coeﬃcients are embedded in the k-th row of pixels. When the size of the coeﬃcients of each\\norder is 2l, the row of pixels is split into 2l areas, each of which is ﬁlled with the same coeﬃcient\\nvalue (Fig 2). By treating these monochrome images as luminance channels for the three primary\\ncolors (red, green, and blue), we synthesize a color image from the trio of monochrome images. The\\nsynthesized color image represents a wavelet-transformed time series of price log returns, spreads,\\nand trading volumes. This conversion from three time series data in one interval to a single color\\nimage is applied to multiple intervals, resulting in multiple color images as the training dataset.\\nThe culmination of this process involves the training of DDPMs using these color images converted\\nfrom time series data. The synthetic images produced by the trained model are then converted back\\ninto time series data, employing the reverse operations of the preceding steps. Fig 3 summarizes\\nthese procedures. Through this process, we demonstrate how our approach generates synthetic\\nﬁnancial time series data by leveraging the capabilities of DDPMs in learning and reproducing the\\ncomplex dynamics of ﬁnancial markets.\\nModel\\nsetup:\\nDDPMs\\nutilize\\na\\nUNet\\narchitecture\\nto\\nfacilitate\\nlearning\\nthrough\\nconvolutional\\nprocesses.\\nThis\\nUNet\\nis\\ncomposed\\nof\\nmulti-stage\\nconvolutions\\nthat\\nin-\\nclude\\nan\\nattention\\nmechanism.\\nFor\\nthis\\nimplementation,\\nwe\\nemploy\\nthe\\nidentical\\nchannel\\ndimension\\nparameters\\nas\\nfound\\nin\\nthe\\nHugging\\nFace\\ntutorial\\non\\nDDPMs\\n(https://huggingface.co/docs/diffusers/tutorials/basic training):\\n128-128-256-256-\\nFigure 2. Pixel imaging. Wavelet coeﬃcients\\nc00, c10, c20, c21, c30, c31, c32, c33, · · · are tiled to image pixels.\\nFigure 3. Overview of the methodology Pixel ﬁlling by wavelet coeﬃcients, overlaying of RGB colors,\\nand noising/denoising of time series images: top left: log returns, middle left: spreads, bottom left: trading\\nvolumes.\\n512. These parameters deﬁne the channel dimensions at various UNet stages, enabling eﬃcient\\nprocessing and feature extraction across diﬀerent scales of the input data.\\n4.\\nResults\\n4.1.\\nExperiments\\nWe train the diﬀusion model to output 2-dimensional, 3-channel images that are converted from\\nthe time series of price log returns, spreads, and trading volumes by wavelet transformation.\\nData: In our experiment, we selected minute-based stock prices, spreads, and trading volumes\\nof AAPL.O traded on NASDAQ from January 2005 to December 2014. We purchased the data\\nfrom Reﬁnitiv (LSEG), a ﬁnancial data provider. The data used in this study consist of minute-\\nlevel OHLC (Open, High, Low, Close) prices for both bids and asks as well as minute-level trading\\nvolumes. During this period, some trading days have minutes without any trades, and we omit such\\nentire trading days in these cases, resulting in 2,481 sample days within this period. One business\\nday opens at 9:30 and closes at 16:00 EST, and we focus on the granularity of these 390 minutes\\nwithin each trading day. Because the open and close times are ﬁxed during this period, the lengths\\nof one-day time series are identical across the samples. After mirror expansion in preprocessing,\\nthe length of the time series becomes 512. Through wavelet transformation and imaging the three\\ntime series, the price log returns, the spreads, and the trading volumes, these time series in one\\nday become 16x256 images with three channels.\\nParameters:\\nIn\\npreprocessing,\\nwe\\nadopt\\npower\\nconversion\\nparameter\\np\\n=\\n1.5\\nfor\\nprice\\nlog\\nreturns\\nand\\np\\n=\\n1.0\\nfor\\nthe\\nspreads\\nand\\ntrading\\nvolumes.\\nThe\\nwin-\\nsorization\\nlevel\\nis\\nset\\nat\\n10.0,\\nmeaning\\nthat\\noutliers\\nbeyond\\n10σ\\nare\\nreplaced\\nby\\n10σ.\\nThe\\nbaseline\\nlogic\\nis\\nidentical\\nto\\nthe\\ntutorial\\nimplementations\\non\\nHugging\\nFace\\n(https://huggingface.co/docs/diffusers/tutorials/basic training). We train the model\\nfor 100 epochs, and other parameters follow the tutorial for DDPMs on Hugging Face.\\nComputational time: Our approach based on DDPM and wavelet transformation takes two\\nhours for training under the above settings and another two hours to generate 2500 images on an\\nNVIDIA GeForce RTX 4090 using the PyTorch framework.\\nFigure 4. Losses in training and validation dataset.\\nComparison to other methodologies: Our methodology compares the outcomes of our\\nDDPMs against established generative models for time series, such as TimeGAN (Yoon et al.\\n2019) and QuantGAN (Wiese et al. 2020). Regarding the DDPM-based approach, we compared it\\nwith discrete wavelet transformation and a simple approach, which ﬁlls 1x512 matrices with time\\nseries data after preprocessing and regards the three 1x512 matrices as one 1x512 color image.\\nThe comparison examines the essential stylized facts of ﬁnancial markets, including their ability to\\nreplicate fat-tailed distributions, the slow decay of the autocorrelations of time series of volatilities,\\nspreads and trading volumes, and the characteristic U-shaped pattern of intraday time series. In\\naddition to these examinations, we also investigated the cross correlation coeﬃcients among time\\nseries.\\n4.2.\\nEvaluation\\nLosses in training and validation: Due to the nature of machine learning models, the values of\\na loss function decrease with regard to the number of epochs. If the values converge, the model’s\\ntraining works well. In this evaluation, we shuﬄed the original samples with 2,481 days. 2,000 days\\nof data are regarded as the training dataset and 481 days of data are the validation dataset. Fig 4\\nexplains the convergence of the loss function of the DDPM and the training is processed well.\\nShape of time series: The comparison among TimeGAN, QuantGAN, and DDPMs (both\\nwith/without wavelet) reveals that TimeGAN’s synthetic time series fail to convincingly replicate\\nthe nuanced movements of the log returns of the stock prices, a deﬁciency that was not rectiﬁed\\nby such parameter adjustments as altering the dimensionality of the latent space (Fig 5). This\\nobservation convinced us to focus our comparative analysis on QuantGAN and our DDPM-based\\napproach in terms of log returns. To the best of our knowledge, no prior studies have generated\\ntime series data on spreads and trading volumes.\\nFat-tailed distribution: The fat-tailed distribution analysis, illustrated in the probability den-\\nsity functions of the log returns (Fig 6), demonstrates our approach’s superiority when mirroring\\nreal distributions up to 10σ. QuantGAN, which uses a ’gaussianizer’ in preprocessing to convert the\\noriginal log return distribution to a Gaussian, shows worse ﬁtting around 1˜2σ compared to the real\\ndata, although it ﬁts well around 10σ. Our approach, based on DDPM and wavelet transformation,\\nalso shows good ﬁts up to 10σ for the spreads and trading volumes.\\nSlow decay of autocorrelation: Some stylized facts pertain to the features of autocorrelations.\\nIn a previous work Cont (2001), the fast decay of autocorrelations is cited as a feature of log returns,\\nand volatility clustering, which quantiﬁes that high-volatility situations tend to cluster in time, is\\nregarded as positive autocorrelations of volatilities and their slow decay. Here we show that our\\nFigure 5. Shapes of time series of log returns. (a) real data as of January 7, 2005, (b) synthetic data\\nby TimeGAN, (c) synthetic data by QuantGAN, (d) synthetic data by DDPM with wavelet imaging, and\\n(e) synthetic data by DDPM without wavelet.\\napproach using DDPM and wavelet transformation explains both the fast decay of autocorrelations\\nin the log returns and the positive autocorrelations with the slow decay in volatilities (Fig 7). In\\nminute-based time series, we also expect similar positive autocorrelation structures and their slow\\ndecays in the spreads and the trading volumes. Our approach also explains the existence of such\\npositive autocorrelations and their slow decays.\\nIntraday seasonality: Furthermore, our analysis of intraday seasonality, which captures the\\npatterns of the volatilities, the spreads, and the trading volumes, conﬁrms the U-shaped pattern\\nwithin a day. Fig 8 represents the average taken every minute for the given time series in the real\\ndata and for the generated time series. This pattern, starting with high values at the market’s\\nopening, dipping mid-day, and rising towards the close, is replicated by our approach, aﬃrming\\nits capacity to accurately mimic real market behaviors. QuantGAN and the simple approach using\\nDDPM without wavelet transformation show ﬂat patterns with regard to time, highlighting their\\nlimitations when representing intraday seasonality observed in real markets.\\nCross correlation coeﬃcients among time series: Because our data are synchronized time\\nseries observed within a day, we expect them to exhibit cross correlations. Table 1 is the cross cor-\\nrelation matrix among time series of real data. A positive correlation coeﬃcient between volatility\\ntime series and trading volume time series indicates that high volatility and high trading volume\\ntend to be observed simultaneously. A negative correlation coeﬃcient, between spread time series\\nand trading volume time series, suggests that market participants actively trade higher volumes\\nunder tight spreads. Table 2 represents the cross correlation matrix among time series generated by\\nDDPM with wavelet imaging, and Table 3 shows the cross correlation matrix among time series by\\nsimple DDPM without wavelet imaging. Both DDPM-based approaches successfully replicate cross\\ncorrelation coeﬃcients among time series observed in real data: a positive correlation coeﬃcient\\nbetween volatilities and trading volumes, a negative correlation coeﬃcient between the spreads and\\nthe trading volumes, another negative correlation coeﬃcient between volatilities and spreads, and\\nother quite small correlations.\\nThese ﬁndings demonstrate the robustness of our DDPM-based method for generating synthetic\\nﬁnancial time series that faithfully reproduce complex market dynamics and stylized facts. These\\ncheck points are summarized in Table 4. Because TimeGAN failed to replicate the nuanced move-\\nments of the log returns of stock prices, we did not check other points. QuantGAN generated only\\nthe log returns of the stock prices; the cross correlations among multiple time series were skipped.\\nFigure 6. Probability density functions. (a) log returns of real data (black) and QuantGAN (blue),\\n(b) log returns of real data (black), DDPM with wavelet imaging (red), and DDPM without wavelet\\n(green), (c) spreads of real data (black), DDPM with wavelet imaging (red), and DDPM without wavelet\\n(green), and (d) trading volumes of real data (black), DDPM with wavelet imaging (red), and DDPM\\nwithout wavelet (green).\\nTable 1. Cross correlations among time series of\\nreal data\\nLog Returns\\nVolatilities\\nSpreads\\nTrading Volumes\\nLog Returns\\n1\\n-0.02\\n0.00\\n-0.02\\nVolatilities\\n1\\n-0.05\\n0.44\\nSpreads\\n1\\n-0.13\\nTrading Volumes\\n1\\nTable 2. Cross correlations among time series of\\nsynthetic data (DDPM+Wavelet)\\nLog Returns\\nVolatilities\\nSpreads\\nTrading Volumes\\nLog Returns\\n1\\n0.00\\n0.01\\n0.00\\nVolatilities\\n1\\n-0.05\\n0.25\\nSpreads\\n1\\n-0.12\\nTrading Volumes\\n1\\nTable 3. Cross correlations among time series of\\nsynthetic data (DDPM+WithoutWavelet)\\nLog Returns\\nVolatilities\\nSpreads\\nTrading Volumes\\nLog Returns\\n1\\n-0.01\\n0.00\\n0.00\\nVolatilities\\n1\\n-0.05\\n0.39\\nSpreads\\n1\\n-0.14\\nTrading Volumes\\n1\\nFigure 7. Autocorrelations. (a) log returns, (b) volatilities, absolute values of log returns, (c) spreads,\\nand (d) trading volumes. In each chart, black chart represents real data, blue chart represents QuantGAN,\\nred chart represents DDPM with wavelet imaging, and green chart represents DDPM without wavelet.\\nFigure 8. Intraday seasonality as average of per-minute time series over samples. (a) volatilities,\\nabsolute value of log return, (b) spreads, and (c) trading volume. In each chart, black chart represents real data,\\nblue chart represents QuantGAN, red chart represents DDPM with wavelet imaging, and green chart represents\\nDDPM without wavelet.\\nTable 4. Summary of comparisons among\\napproaches\\nTimeGAN\\nQuantGAN\\nDDPM (without wavelet)\\nDDPM+Wavelet\\nShape of time series\\nNG\\nOK\\nOK\\nOK\\nFat tail\\n-\\nOK\\nOK\\nOK\\nSlow decays of autocorrelation\\n-\\nOK\\nOK\\nOK\\nIntraday seasonality pattern\\n-\\nNG\\nNG\\nOK\\nCross correlation function\\n-\\n-\\nOK\\nOK\\n5.\\nConclusions\\nWe suggested an alternative approach to generate synthetic time series by wavelet transformation\\nand denoising diﬀusion probabilistic model (DDPM). The DDPM and wavelet imaging approaches\\nmore eﬀectively replicated characteristics commonly observed in ﬁnancial time series, such as the\\nfat tails, the slow decay of autocorrelations including volatility clustering, the intraday seasonal-\\nity patterns, and the cross correlation coeﬃcients among time series, compared to TimeGAN and\\nQuantGAN approaches and the simple application of DDPM without wavelet imaging. The DDPM\\nand wavelet approach especially had a better representation of intraday seasonality in the actual\\nmarket data than these methodologies in comparison. Imaging through the wavelet transformation\\ncan capture intraday seasonality because the relationship among frequencies is more explicit than\\nthe original time series. In the preprocessing of imaging, the DDPM with a wavelet imaging ap-\\nproach ﬁlls the top rows of an image with a zero-th wavelet coeﬃcient, which represents the overall\\nintraday trend. The next and subsequent pixel rows of the image represent progressively ﬁner mar-\\nket microstructures. As a result, the information of short-term microstructures with multiple time\\nscales by wavelet transformation around market open (resp. close) is embedded in the left (resp.\\nright) side of the image. This contributes to the representation of the U-shape structures of the\\nobserved intraday data.\\nIn conclusion, our application of denoising diﬀusion probabilistic models (DDPMs) in conjunction\\nwith wavelet image transformation was an eﬀective method for generating synthetic ﬁnancial time\\nseries that closely adhere to underlying stylized facts. The strategic use of RGB channels in color\\nimages to represent and simultaneously generate three interconnected time series replicated the\\nstructure of the cross correlation functions among the multiple time series, representing a signiﬁcant\\nadvancement in the ﬁeld. Looking ahead, the potential to extend this methodology to utilize three\\nor more channels might allow simultaneous generation of multiple correlated stock prices. Building\\non this foundation, future work is poised to explore the generation of synthetic data that capture\\neven more nuanced relationships and cross correlations within ﬁnancial markets, extending the\\nboundaries of what is possible in synthetic data generation and ﬁnancial modeling.\\nReferences\\nBachelier, L., Th´eorie de la sp´eculation. Annales scientiﬁque de l’E.N.S. (SMF), 1900, 17, 21-86.\\nBellemare, M.F. and Wichman, C.J., Elasticities and the Inverse Hyperbolic Sine Transformation. Oxf. Bull.\\nEcon. Stat., 2020, 82 (1), 50-61.\\nBollerslev, T., Chou, R.Y. and Kroner, K.F., ARCH Modeling in Finance: A review of the theory and\\nempirical evidence. J. Econom., 1992, 52 (1-2), 5-59.\\nBrophy, E., Wang, Z., She, Q. and Ward, T., Generative Adversarial Networks in Time Series: A Systematic\\nLiterature Review. ACM Comput. Surv., 2023, 55 (10) 199, 1-31.\\nChakraborti, A., Patriarca, M. and Santhanam, MS., Financial Time-Series Analysis: A Brief Overview. In\\nEconophysics of Markets and Business Networks, edited by A. Chatterjee and B.K. Chakrabarti, pp.51-67,\\n2007 (Springer Milan).\\nCont, R., Empirical Properties of Asset Returns: Stylized Facts and Statistical Issues. Quant. Finance, 2001,\\n1 (2), 223-236.\\nDas, A., Kong, W., Sen, R. and Zhou, Y., A Decoder-Only Foundation Model for Time-Series Forecasting,\\n2023, Available online at: https://arxiv.org/abs/2310.10688 (accessed 27 September 2024)\\nDogariu, M., S¸tefan, L.D., Boteanu, B.A., Lamba, C., Kim, B. and Ionescu, B., Generation of Realistic\\nSynthetic Financial Time-Series. ACM Trans. Multimedia Comput. Commun. Appl., 2022, 18 (4) 96,\\n1-27.\\nDonahue, C., McAuley, J. and Puckette, M., Adversarial Audio Synthesis. Paper presented at International\\nConference on Learning Representation (ICLR 2019).\\nEckerli, F. and Osterrieder, J., Generative Adversarial Networks in Finance: An Overview, 2021, Available\\nonline at: https://papers.ssrn.com/abstract=3864965 (accessed 27 September 2024)\\nGabaix, X., Gopikrishnan, P., Plerou, V. and Stanley, H.E., A Theory of Power-Law Distributions in Finan-\\ncial Market Fluctuations. Nature, 2003, 423 (6937), 267-270.\\nGabaix, X., Power Laws in Economics and Finance. Annu. Rev. Econom., 2009, 1, 255-294.\\nGoodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio,\\nY., Generative Adversarial Nets. Paper presented at the International Conference on Neural Information\\nProcessing Systems (NIPS 2014), 2672-2680, 2014.\\nHo, J., Jain, A. and Abbeel, P., Denoising Diﬀusion Probabilistic Models. Paper presented at the 34th\\nInternational Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada,\\n2020.\\nKingma, D.P. and Welling, M., Auto-Encoding Variational Bayes. Paper presented at the International\\nConference on Learning Representation (ICLR 2014), 2014.\\nLux, T., Stochastic Behavioral Asset-Pricing Models and the Stylized Facts. In Handbook of Financial\\nMarkets: Dynamics and Evolution, edited by T. Hens and K.R. Schenk-Hoppe, pp.161-215, 2009 (Science\\nDirect).\\nMizuno, T., Kurihara, S., Takayasu, M. and Takayasu, H., Analysis of High-Resolution Foreign Exchange\\nData of USD-JPY for 13 Years. Physica A, 2003, 324 (1-2), 296-302.\\nPlerou, V., Gopikrishnan, P., Rosenow, B., Amaral, L.A. and Stanley, H.E., Econophysics: Financial Time\\nSeries from a Statistical Physics Point of View. Physica A, 2000, 279 (1-4), 443-456.\\nRamsey, J.B., Usikov, D. and Zaslavsky, G.M., AN ANALYSIS OF U.S. STOCK PRICE BEHAVIOR\\nUSING WAVELETS. Fractals, 1995, 03 (02), 377-389.\\nRatliﬀ-Crain, E., Van Oort, C.M., Bagrow, J., Koehler, M.T. and Tivnan, B.F., Revisiting Stylized Facts for\\nModern Stock Markets. Paper presented at 2023 IEEE International Conference on Big Data (BigData),\\nSorrento, Italy, 15-18 December, 2023.\\nSamanidou, E., Zschischang, E., Stauﬀer, D. and Lux, T., Agent-Based Models of Financial Markets. Rep.\\nProg. Phys., 2007, 70 (3).\\nShakeel, M. and Srivastava, B., Stylized Facts of High-Frequency Financial Time Series Data. Global Bus.\\nRev., 2018, 22 (2), 550-564.\\nTakayasu, M., Watanabe, K., Mizuno, T. and Takayasu, H., Theoretical Base of the PUCK-Model with\\nApplication to Foreign Exchange Markets. In Econophysics Approaches to Large-Scale Business Data and\\nFinancial Crisis, edited by M. Takayasu, T. Watanabe and H. Takayasu, pp.79-98, 2010 (Springer Tokyo).\\nTashiro, Y., Song, J., Song, Y. and Ermon, S., CSDI: Conditional Score-Based Diﬀusion Models for Proba-\\nbilistic Time Series Imputation. Paper presented at the 35th Conference on Neural Information Processing\\nSystems (NeurIPS 2021), 2021.\\nWiese, M., Knobloch, R., Korn, R. and Kretschmer, P., Quant GANs: Deep Generation of Financial Time\\nSeries. Quant. Finance, 2020, 20 (9), 1419-1440.\\nXiao, Z., Kreis, K. and Vahdat, A., Tackling the Generative Learning Trilemma with Denoising Diﬀusion\\nGANs. Paper presented at the International Conference on Learning Representations (ICLR 2022), 25-29\\nApril 2022.\\nYoon, J., Jarrett, D. and van der Schaar, M., Time-Series Generative Adversarial Networks. Paper pre-\\nsented at the 33rd International Conference on Neural Information Processing Systems (NeurIPS 2019),\\nVancouver, Canada, 2019.\\n')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loaded all the research paper\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd98cd96-f347-46ff-a0d7-5d47f76ea840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yuvraj pandey\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11705 sha256=1ee0ca08cdb61cd9f5a3285c503bc8b37a88aedf88ad321e7abcf421ff72a0ea\n",
      "  Stored in directory: c:\\users\\yuvraj pandey\\appdata\\local\\pip\\cache\\wheels\\63\\47\\7c\\a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb098689-4809-432c-a1da-168e0f67d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Financial forecast', 'summary': 'A financial forecast is an estimate of future financial outcomes for a company or project, usually applied in budgeting, capital budgeting and / or valuation. Depending on context, the term may also refer to listed company (quarterly) earnings guidance.\\nFor a country or economy, see Economic forecast.\\nTypically, using historical internal accounting and sales data, in addition to external industry data and economic indicators, a financial forecast will be the analyst\\'s modeled prediction of company outcomes in financial terms over a given time period. \\nFor the components / steps of business modeling here, see Outline of finance § Financial modeling. \\nArguably, the key aspect of preparing a financial forecast is predicting revenue; \\nfuture costs, fixed and variable, as well as capital, can then be estimated as a function of sales via \"common-sized analysis\" - where relationships are derived from historical financial ratios and other accounting relationships. At the same time, the resultant line items must talk to the business\\' operations:- in general, growth in revenue will require corresponding increases in working capital, fixed assets (see, here, owner earnings)\\nand associated financing; and in the long term, profitability (and other financial ratios) should tend to the industry average; \\nsee Valuation using discounted cash flows  § Determine cash flow for each forecast period for more detailed discussion, and other considerations; also Cash flow forecasting.\\nThere is an extensive literature on the accuracy of analyst forecasts of revenue, profit and share price developments of companies. In general, this literature shows that analysts do not produce better forecasts than simple forecasting models.\\n(Additional to the above outline, for fundamental analysis, analysts often also use stock market information, such as the 52-week high of stock prices, to augment their analysis of stock prices.) \\n\\n', 'source': 'https://en.wikipedia.org/wiki/Financial_forecast'}, page_content='A financial forecast is an estimate of future financial outcomes for a company or project, usually applied in budgeting, capital budgeting and / or valuation. Depending on context, the term may also refer to listed company (quarterly) earnings guidance.\\nFor a country or economy, see Economic forecast.\\nTypically, using historical internal accounting and sales data, in addition to external industry data and economic indicators, a financial forecast will be the analyst\\'s modeled prediction of company outcomes in financial terms over a given time period. \\nFor the components / steps of business modeling here, see Outline of finance § Financial modeling. \\nArguably, the key aspect of preparing a financial forecast is predicting revenue; \\nfuture costs, fixed and variable, as well as capital, can then be estimated as a function of sales via \"common-sized analysis\" - where relationships are derived from historical financial ratios and other accounting relationships. At the same time, the resultant line items must talk to the business\\' operations:- in general, growth in revenue will require corresponding increases in working capital, fixed assets (see, here, owner earnings)\\nand associated financing; and in the long term, profitability (and other financial ratios) should tend to the industry average; \\nsee Valuation using discounted cash flows  § Determine cash flow for each forecast period for more detailed discussion, and other considerations; also Cash flow forecasting.\\nThere is an extensive literature on the accuracy of analyst forecasts of revenue, profit and share price developments of companies. In general, this literature shows that analysts do not produce better forecasts than simple forecasting models.\\n(Additional to the above outline, for fundamental analysis, analysts often also use stock market information, such as the 52-week high of stock prices, to augment their analysis of stock prices.) \\n\\n\\n== References =='), Document(metadata={'title': 'Financial analysis', 'summary': 'Financial analysis (also known as financial statement analysis, accounting analysis, or analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. \\nIt is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  \\nFinancial analysis may determine if a business will:\\n\\nContinue or discontinue its main operation or part of its business;\\nMake or purchase certain materials in the manufacture of its product;\\nAcquire or rent/lease certain machineries and equipment in the production of its goods;\\nIssue  shares or negotiate for a bank loan to increase its working capital;\\nMake decisions regarding investing or lending capital;\\nMake other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.', 'source': 'https://en.wikipedia.org/wiki/Financial_analysis'}, page_content='Financial analysis (also known as financial statement analysis, accounting analysis, or analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. \\nIt is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  \\nFinancial analysis may determine if a business will:\\n\\nContinue or discontinue its main operation or part of its business;\\nMake or purchase certain materials in the manufacture of its product;\\nAcquire or rent/lease certain machineries and equipment in the production of its goods;\\nIssue  shares or negotiate for a bank loan to increase its working capital;\\nMake decisions regarding investing or lending capital;\\nMake other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.\\n\\n\\n== Firm-level analysis ==\\nFinancial analysts often assess the following elements of a firm:\\n\\nProfitability - its ability to earn income and sustain growth in both the short- and long-term. A company\\'s degree of profitability is usually based on the income statement, which reports on the company\\'s results of operations;\\nSolvency - its ability to pay its obligation to creditors and other third parties in the long-term;\\nLiquidity - its ability to maintain positive cash flow, while satisfying immediate obligations;\\nStability - the firm\\'s ability to remain in business in the long run, without having to sustain significant losses in the conduct of its business. Assessing a company\\'s stability requires the use of both the income statement and the balance sheet, as well as other financial and non-financial indicators.\\nBoth 2 and 3 are based on the company\\'s balance sheet, which indicates the financial condition of a business as of a given point in time.\\n\\n\\n== Techniques ==\\nFinancial analysts often compare financial ratios (of solvency, profitability, growth, etc.):\\n\\nPast Performance - Across historical time periods for the same firm (the last 5 years for example),\\nFuture Performance - Using historical figures and certain mathematical and statistical techniques, including present and future values, This extrapolation method is the main source of errors in financial analysis as past statistics can be poor predictors of future prospects.\\nComparative Performance - Comparison between similar firms\\nComparing financial ratios is merely one way of conducting financial analysis. \\nFinancial analysts can also use percentage analysis which involves reducing a series of figures as a percentage of some base amount.  For example, a group of items can be expressed as a percentage of net income. When proportionate changes in the same figure over a given time period expressed as a percentage is known as horizontal analysis.\\nVertical or common-size analysis reduces all items on a statement to a \"common size\" as a percentage of some base value which assists in comparability with other companies of different sizes. As a result, all Income Statement items are divided by Sales, and all Balance Sheet items are divided by Total Assets.\\nAnother method is comparative analysis. This provides a better way to determine trends. Comparative analysis presents the same information for two or more time periods and is presented side-by-side to allow for easy analysis.\\n\\n\\n== Theoretical challenges ==\\nFinancial ratios face several theoretical challenges:\\n\\nThey say little about the firm\\'s prospects in an absolute sense.  Their insights about relative performance require a reference point from other time periods or similar firms.\\nOne ratio holds little meaning.  As indicators, ratios can be logically interpreted in at least two ways. One can partially overcome this problem by combining several related ratios to paint a more comprehensive picture of the firm')]\n"
     ]
    }
   ],
   "source": [
    "## load data from wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Financial forecast\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28a8017a-8b74-49c3-9301-bb1ac04f3f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Financial forecast', 'summary': 'A financial forecast is an estimate of future financial outcomes for a company or project, usually applied in budgeting, capital budgeting and / or valuation. Depending on context, the term may also refer to listed company (quarterly) earnings guidance.\\nFor a country or economy, see Economic forecast.\\nTypically, using historical internal accounting and sales data, in addition to external industry data and economic indicators, a financial forecast will be the analyst\\'s modeled prediction of company outcomes in financial terms over a given time period. \\nFor the components / steps of business modeling here, see Outline of finance § Financial modeling. \\nArguably, the key aspect of preparing a financial forecast is predicting revenue; \\nfuture costs, fixed and variable, as well as capital, can then be estimated as a function of sales via \"common-sized analysis\" - where relationships are derived from historical financial ratios and other accounting relationships. At the same time, the resultant line items must talk to the business\\' operations:- in general, growth in revenue will require corresponding increases in working capital, fixed assets (see, here, owner earnings)\\nand associated financing; and in the long term, profitability (and other financial ratios) should tend to the industry average; \\nsee Valuation using discounted cash flows  § Determine cash flow for each forecast period for more detailed discussion, and other considerations; also Cash flow forecasting.\\nThere is an extensive literature on the accuracy of analyst forecasts of revenue, profit and share price developments of companies. In general, this literature shows that analysts do not produce better forecasts than simple forecasting models.\\n(Additional to the above outline, for fundamental analysis, analysts often also use stock market information, such as the 52-week high of stock prices, to augment their analysis of stock prices.) \\n\\n', 'source': 'https://en.wikipedia.org/wiki/Financial_forecast'}, page_content='A financial forecast is an estimate of future financial outcomes for a company or project, usually applied in budgeting, capital budgeting and / or valuation. Depending on context, the term may also refer to listed company (quarterly) earnings guidance.\\nFor a country or economy, see Economic forecast.\\nTypically, using historical internal accounting and sales data, in addition to external industry data and economic indicators, a financial forecast will be the analyst\\'s modeled prediction of company outcomes in financial terms over a given time period. \\nFor the components / steps of business modeling here, see Outline of finance § Financial modeling. \\nArguably, the key aspect of preparing a financial forecast is predicting revenue; \\nfuture costs, fixed and variable, as well as capital, can then be estimated as a function of sales via \"common-sized analysis\" - where relationships are derived from historical financial ratios and other accounting relationships. At the same time, the resultant line items must talk to the business\\' operations:- in general, growth in revenue will require corresponding increases in working capital, fixed assets (see, here, owner earnings)\\nand associated financing; and in the long term, profitability (and other financial ratios) should tend to the industry average; \\nsee Valuation using discounted cash flows  § Determine cash flow for each forecast period for more detailed discussion, and other considerations; also Cash flow forecasting.\\nThere is an extensive literature on the accuracy of analyst forecasts of revenue, profit and share price developments of companies. In general, this literature shows that analysts do not produce better forecasts than simple forecasting models.\\n(Additional to the above outline, for fundamental analysis, analysts often also use stock market information, such as the 52-week high of stock prices, to augment their analysis of stock prices.) \\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Financial analysis', 'summary': 'Financial analysis (also known as financial statement analysis, accounting analysis, or analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. \\nIt is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  \\nFinancial analysis may determine if a business will:\\n\\nContinue or discontinue its main operation or part of its business;\\nMake or purchase certain materials in the manufacture of its product;\\nAcquire or rent/lease certain machineries and equipment in the production of its goods;\\nIssue  shares or negotiate for a bank loan to increase its working capital;\\nMake decisions regarding investing or lending capital;\\nMake other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.', 'source': 'https://en.wikipedia.org/wiki/Financial_analysis'}, page_content='Financial analysis (also known as financial statement analysis, accounting analysis, or analysis of finance) refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. \\nIt is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions.  \\nFinancial analysis may determine if a business will:\\n\\nContinue or discontinue its main operation or part of its business;\\nMake or purchase certain materials in the manufacture of its product;\\nAcquire or rent/lease certain machineries and equipment in the production of its goods;\\nIssue  shares or negotiate for a bank loan to increase its working capital;\\nMake decisions regarding investing or lending capital;\\nMake other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.\\n\\n\\n== Firm-level analysis ==\\nFinancial analysts often assess the following elements of a firm:\\n\\nProfitability - its ability to earn income and sustain growth in both the short- and long-term. A company\\'s degree of profitability is usually based on the income statement, which reports on the company\\'s results of operations;\\nSolvency - its ability to pay its obligation to creditors and other third parties in the long-term;\\nLiquidity - its ability to maintain positive cash flow, while satisfying immediate obligations;\\nStability - the firm\\'s ability to remain in business in the long run, without having to sustain significant losses in the conduct of its business. Assessing a company\\'s stability requires the use of both the income statement and the balance sheet, as well as other financial and non-financial indicators.\\nBoth 2 and 3 are based on the company\\'s balance sheet, which indicates the financial condition of a business as of a given point in time.\\n\\n\\n== Techniques ==\\nFinancial analysts often compare financial ratios (of solvency, profitability, growth, etc.):\\n\\nPast Performance - Across historical time periods for the same firm (the last 5 years for example),\\nFuture Performance - Using historical figures and certain mathematical and statistical techniques, including present and future values, This extrapolation method is the main source of errors in financial analysis as past statistics can be poor predictors of future prospects.\\nComparative Performance - Comparison between similar firms\\nComparing financial ratios is merely one way of conducting financial analysis. \\nFinancial analysts can also use percentage analysis which involves reducing a series of figures as a percentage of some base amount.  For example, a group of items can be expressed as a percentage of net income. When proportionate changes in the same figure over a given time period expressed as a percentage is known as horizontal analysis.\\nVertical or common-size analysis reduces all items on a statement to a \"common size\" as a percentage of some base value which assists in comparability with other companies of different sizes. As a result, all Income Statement items are divided by Sales, and all Balance Sheet items are divided by Total Assets.\\nAnother method is comparative analysis. This provides a better way to determine trends. Comparative analysis presents the same information for two or more time periods and is presented side-by-side to allow for easy analysis.\\n\\n\\n== Theoretical challenges ==\\nFinancial ratios face several theoretical challenges:\\n\\nThey say little about the firm\\'s prospects in an absolute sense.  Their insights about relative performance require a reference point from other time periods or similar firms.\\nOne ratio holds little meaning.  As indicators, ratios can be logically interpreted in at least two ways. One can partially overcome this problem by combining several related ratios to paint a more comprehensive picture of the firm')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168cdc2-1119-4c45-a4da-32995bb72318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
